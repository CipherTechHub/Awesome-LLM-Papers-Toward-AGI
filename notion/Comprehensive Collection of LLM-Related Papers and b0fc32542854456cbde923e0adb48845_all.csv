Impact,Model,Title,Category,ArXiv Link,GitHub,Citation,Publication Date,Foundation,Property,ID,ImageURL,Connect,In a nut shell,SemanticScholar
,,StarCoder 2 and The Stack v2: The Next Generation,Code-LLM,,,,,,,260,,,,
,,TaskWeaver: A Code-First Agent Framework,"Agent, Code-LLM",,,,,,,264,,,,
,,Creative Robot Tool Use with Large Language Models,"Code-as-Policies, Robot",,,,,,,359,,,,
,,RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation,"Code-as-Policies, Robot",,,,,,,358,,,,
,,Executable Code Actions Elicit Better LLM Agents,"Code-as-Policies, Robot",,,,,,,317,,,,
,,Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought,"Chain-of-Thought, Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2305.16744,,,,,,310,,,,
,,Executable Code Actions Elicit Better LLM Agents,"Agent, Code-as-Policies",https://arxiv.org/abs/2402.01030,,,2024/01/24,,,294,,,,
,,Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Code-as-Policies, Reasoning",,,,,,,293,,,,
,,RoboCodeX:Multi-modal Code Generation forRobotic Behavior Synthesis,"Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2402.16117,,,,,,290,,,,
,,Code as Reward: Empowering Reinforcement Learning with VLMs,"Code-as-Policies, Reinforcement-Learning, Reward",https://arxiv.org/abs/2402.04764,,,,,,259,,,,
,Code as policies,Code as Policies: Language Model Programs for Embodied Control,"Code-as-Policies, Embodied, PersonalCitation, Robot",https://arxiv.org/abs/2209.07753,,,2022/09/16,,Code generation,10,https://code-as-policies.github.io/img/share_image.png,,,
,SMART-LLM,SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"Code-as-Policies, Robot",https://arxiv.org/abs/2309.10062,,,2023/09/18,,Code generation,56,,,,
,ViperGPT,ViperGPT: Visual Inference via Python Execution for Reasoning,"Code-as-Policies, Reasoning, VLM, VQA",https://arxiv.org/abs/2303.08128,,,2023/03/14,,Visual Question Answering,4,,,,
,Socratic,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,"Code-as-Policies, PersonalCitation, Robot, Zero-shot",https://arxiv.org/abs/2204.00598,,,2022/04/01,,Code generation,90,,,,
,Progprompt,ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,"Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2209.11302,,,2022/09/22,,Code generation,95,,,,
,Statler,Statler: State-Maintaining Language Models for Embodied Reasoning,"Code-as-Policies, PersonalCitation, Robot, State-Manage",https://arxiv.org/abs/2306.17840,,,2023/06/30,,Code generation,100,,,,
,VISPROG,Visual Programming: Compositional visual reasoning without training,"Code-as-Policies, VLM, VQA",https://arxiv.org/abs/2211.11559,,,2022/11/18,,Visual Question Answering,99,,,,
,Instruct2Act,Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model,"Code-as-Policies, Multimodal, OpenGVLab, PersonalCitation, Robot",https://arxiv.org/abs/2305.11176,,,2023/05/18,,Multimodal prompts,105,,,,
,,RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks,"Code-as-Policies, PersonalCitation, Robot",,,,,,,360,,,,
,,Inner Monologue: Embodied Reasoning through Planning with Language Models,"Code-as-Policies, Embodied, PersonalCitation, Reasoning, Robot, Task-Decompose",https://arxiv.org/abs/2207.05608,,,,,,119,,,,
,,ChatGPT for Robotics: Design Principles and Model Abilities,"Code-as-Policies, PersonalCitation, Robot",,,,,,,361,,,,
,A Survey of Chain of Thought Reasoning,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","Chain-of-Thought, Reasoning, Survey",https://arxiv.org/abs/2309.15402,,,2023/09/27,,Survey Paper,1,,,,
,SelfCheck,SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,"Chain-of-Thought, Planning, Reasoning",https://arxiv.org/abs/2308.00436,,,2023/08/01,,Planning,2,,,,
,Robot Learning,Robot Learning in the Era of Foundation Models: A Survey,"Robot, Survey",https://arxiv.org/abs/2311.14379,,,2023/11/24,,Survey papers,3,,,,
,MOMA-Force,MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation,"Multimodal, Robot",https://arxiv.org/abs/2308.03624,,,2023/08/07,,Multimodal prompts,5,,,,
,Language to Rewards,Language to Rewards for Robotic Skill Synthesis,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2306.08647,,,2023/06/14,,Reinforcement Learning,6,,,,
,Caption Anything,Caption Anything: Interactive Image Description with Diverse Multimodal Controls,"Caption, VLM, VQA",https://arxiv.org/abs/2305.02677,,,2023/05/04,,Visual Question Answering,7,,,,
,ELLM,Guiding Pretraining in Reinforcement Learning with Large Language Models,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2302.06692,,,2023/02/13,,Reinforcement Learning,8,,,,
,InternGPT,InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language,"Intaractive, OpenGVLab, VLM",https://arxiv.org/abs/2305.05662,,,2023/05/09,,Vision-LLM,9,,,,
,The Development of LLMs,The Development of LLMs for Embodied Navigation,"Embodied, LLM, Robot, Survey",https://arxiv.org/abs/2311.00530,,,2023/11/01,,Survey papers,11,,,,
,REFLECT,REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,"Feedback, Robot",https://arxiv.org/abs/2306.15724,,,2023/06/27,,Self-improvement,12,,,,
,Autonomous Agents,A Survey on Large Language Model based Autonomous Agents,"Agent, Survey",https://arxiv.org/abs/2308.11432,,,2023/08/22,,Planning,13,,,,
,DOREMI,DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,"Perception, Task-Decompose",https://arxiv.org/abs/2307.00329,,,2023/07/01,,Decomposing task,14,,,,
50,Chain of Thought,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2201.11903,,,2022/01/28,,Chain of Thought,15,,,,
,AgentSims,AgentSims: An Open-Source Sandbox for Large Language Model Evaluation,Agent,https://arxiv.org/abs/2308.04026,,,2023/08/08,,Open-Source Evaluation,16,,,,
,OWL-ViT,Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,https://arxiv.org/abs/2205.06230,,,2022/05/12,,Object Detection,17,,,,
,GPT4Vis,GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?,"LLM, Zero-shot",https://arxiv.org/abs/2311.15732,,,2023/11/27,,Quantitive Analysis,18,,,,
,Embodied Task Planning,Embodied Task Planning with Large Language Models,"Embodied, Robot, Task-Decompose",https://arxiv.org/abs/2307.01848,,,2023/07/04,,Planning,19,,,,
,Lafite-RL,Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models,"Agent, Feedback, Reinforcement-Learning, Robot",https://arxiv.org/abs/2311.02379,,,2023/11/04,,Reinforcement Learning,20,,,,
,MiniGPT-4,MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,"Instruction-Turning, LLM",https://arxiv.org/abs/2304.10592,,,2023/04/20,,,21,,,,
,Segment Anything,Segment Anything,"LLM, Open-source, Perception, Segmentation",https://arxiv.org/abs/2304.02643,,,2023/04/05,,Object Detection,22,,,,
,RLAdapter,RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds,"Agent, Minecraft, Reinforcement-Learning",,,,,,Reinforcement Learning,23,,,,
,MM-ReAct,MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,"Reasoning, VLM, VQA",https://arxiv.org/abs/2303.11381,,,2023/03/20,,Visual Question Answering,24,,,,
,Least-to-Most Prompting,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2205.10625,,,2022/05/21,,Reasoning,25,,,,
,Verify-and-Edit,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.03268,,,2023/05/05,,Chain of Thought,26,,,,
,JARVIS-1,JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models,"Agent, Memory, Minecraft",https://arxiv.org/abs/2311.05997,,,2023/11/10,,Planning,27,,,,
,BIG-Bench,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,In-Context-Learning,https://arxiv.org/abs/2206.04615,,,2022/06/09,,Benchmark,28,,,,
,Chameleon,Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,"VLM, VQA",https://arxiv.org/abs/2304.09842,,,2023/04/19,,Visual Question Answering,29,,,,
90,Large Language Model Based Agents,The Rise and Potential of Large Language Model Based Agents: A Survey,"Agent, Survey",https://arxiv.org/abs/2309.07864,,,2023/09/14,,Survey Paper,30,,,,
,Prompt a Robot to Walk,Prompt a Robot to Walk with Large Language Models,"Low-level-action, Robot",https://arxiv.org/abs/2309.09969,,,2023/09/18,,Low-level output,31,,,,
,Physically Grounded Vision-Language Model,Physically Grounded Vision-Language Models for Robotic Manipulation,"End2End, Multimodal, Robot",https://arxiv.org/abs/2309.02561,,,2023/09/05,,Multimodal LLM,32,,,,
,GPT4-V OpenFlamingo,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"Open-source, VLM",https://arxiv.org/abs/2308.01390,,,2023/08/02,,Vision-LLM,33,,,,
,Algorithm of Thoughts,Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2308.10379,,,2023/08/20,,Reasoning,34,,,,
,COMPLEXITY-CoT,Complexity-Based Prompting for Multi-Step Reasoning,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2210.00720,,,2022/10/03,,Reasoning,35,,,,
,Self-Ask,Measuring and Narrowing the Compositionality Gap in Language Models,"Chain-of-Thought, In-Context-Learning, Self",https://arxiv.org/abs/2210.03350,,,2022/10/07,,Reasoning,36,,,,
,Self-supervised ICL,SINC: Self-Supervised In-Context Learning for Vision-Language Tasks,"In-Context-Learning, VQA",https://arxiv.org/abs/2307.07742,,,2023/07/15,,Self-supervised,37,,,,
,Toward General-Purpose,Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis,"Robot, Survey",https://arxiv.org/abs/2312.08782,,,2023/12/14,,Survey papers,38,,https://www.connectedpapers.com/main/6140211405f9917ded519da50f00eee989eabd7f/Toward-General Purpose-Robots-via-Foundation-Models%3A-A-Survey-and-Meta Analysis/graph,,
,AdaRefiner,AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback,"Agent, Feedback, Reinforcement-Learning",https://arxiv.org/abs/2309.17176,,,2023/09/29,,Reinforcement Learning,39,,,,
,AgentInstruct,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,"Agent, Reasoning, Zero-shot",https://arxiv.org/abs/2310.03710,,,2023/10/05,,Planning,40,,,,
,MOO,Open-World Object Manipulation using Pre-trained Vision-Language Models,"Multimodal, Robot",https://arxiv.org/abs/2303.00905,,,2023/03/02,,Multimodal LLM,41,,,,
,SuperICL,Small Models are Valuable Plug-ins for Large Language Models,In-Context-Learning,https://arxiv.org/abs/2305.08848,,,2023/05/15,,Reasoning,42,,,,
,LLaVA,Visual Instruction Tuning,"Instruction-Turning, LLM, PEFT",https://arxiv.org/abs/2304.08485,,,2023/04/17,,,43,,,,
,Gemini vs GPT-4V,Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases,"GPT4, Gemini, LLM",https://arxiv.org/abs/2312.15011,,,2023/12/22,,Quantitive Analysis,44,,,,
,OpenFlamingo,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"LLM, Open-source",https://arxiv.org/abs/2308.01390,,,2023/08/02,,Open sourced LLM,45,,,,
,PaLM,PaLM: Scaling Language Modeling with Pathways,VLM,https://arxiv.org/abs/2204.02311,,,2022/04/05,,Vision-LLM,46,,,,
,DEPS,"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","Agent, Minecraft",https://arxiv.org/abs/2302.01560,,,2023/02/03,,Planning,47,,,,
,Reasoning in Large Language Models,Towards Reasoning in Large Language Models: A Survey,"LLM, Reasoning, Survey",https://arxiv.org/abs/2212.10403,,,2022/12/20,,Survey Paper,48,,,,
20,Gpt-driver,GPT-Driver: Learning to Drive with GPT,"Driving, Spacial",https://arxiv.org/abs/2310.01415,,,2023/10/02,,Spatial Understanding,49,,,,
,NL2TL,NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,"LLM, Temporal Logics",https://arxiv.org/abs/2305.07766,,,2023/05/12,,Temporal Logics,50,,,,
,Self-Refine,Self-Refine: Iterative Refinement with Self-Feedback,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2303.17651,,,2023/03/30,,Reasoning,51,,,,
,PAL,PAL: Program-aided Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2211.10435,,,2022/11/18,,Reasoning,52,,,,
,Auto-CoT,Automatic Chain of Thought Prompting in Large Language Models,"Automate, Chain-of-Thought, Reasoning",https://arxiv.org/abs/2210.03493,,,2022/10/07,,Chain of Thought,53,,,,
,PaLM-E,PaLM-E: An Embodied Multimodal Language Model,"End2End, Multimodal, Robot",https://arxiv.org/abs/2303.03378,,,2023/03/06,,Multimodal LLM,54,,,,
,CogVLM,CogVLM: Visual Expert for Pretrained Language Models,"VLM, VQA",https://arxiv.org/abs/2311.03079,,,2023/11/06,,Visual Question Answering,55,,,,
,Path planners,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,"LLM, Spacial",https://arxiv.org/abs/2310.03249,,,2023/10/05,,Spatial Understanding,57,,,,
,A Survey of Large Language Models,A Survey of Large Language Models,"LLM, Survey",https://arxiv.org/abs/2303.18223,,,2023/03/31,,Survey Papers,58,,,,
,VIMA,VIMA: General Robot Manipulation with Multimodal Prompts,"End2End, Multimodal, Robot",https://arxiv.org/abs/2210.03094,,,2022/10/06,,Multimodal prompts,59,,,,
,Maieutic Prompting,Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2205.11822,,,2022/05/24,,Reasoning,60,,,,
,Tree of Thought,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.10601,,,2023/05/17,,Chain of Thought,61,,,,
,Voyager,Voyager: An Open-Ended Embodied Agent with Large Language Models,"Agent, Minecraft",https://arxiv.org/abs/2305.16291,,,2023/05/25,,Planning,62,,,,
,Eureka,Eureka: Human-Level Reward Design via Coding Large Language Models,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2310.12931,,,2023/10/19,,Reinforcement Learning,63,,,,
,Plan-and-Solve,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2305.04091,,,2023/05/06,,Reasoning,64,,,,
,3D-LLM,3D-LLM: Injecting the 3D World into Large Language Models,"3D, Open-source, Perception, Robot",https://arxiv.org/abs/2307.12981,,,2023/07/24,,Multimodal Data injection,65,,,,
,Text2Reward,Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2309.11489,,,2023/09/20,,Reinforcement Learning,66,,,,
,Flamingo,Flamingo: a Visual Language Model for Few-Shot Learning,"Multimodal, Robot",https://arxiv.org/abs/2204.14198,,,2022/04/29,,Multimodal LLM,67,,,,
,Generative Agents,Generative Agents: Interactive Simulacra of Human Behavior,In-Context-Learning,https://arxiv.org/abs/2304.03442,,,2023/04/07,,Memory,68,,,,
,VisualCOMET,VisualCOMET: Reasoning about the Dynamic Context of a Still Image,"In-Context-Learning, VQA",https://arxiv.org/abs/2004.10796,,,2020/04/22,,Reasoning,69,,,,
,Self-Polish,Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,"Chain-of-Thought, In-Context-Learning, Self",https://arxiv.org/abs/2305.14497,,,2023/05/23,,Reasoning,70,,,,
,Gensim,GenSim: Generating Robotic Simulation Tasks via Large Language Models,"Data-generation, Robot",https://arxiv.org/abs/2310.01361,,,2023/10/02,,Data generation,71,,,,
,PointCLIP,PointCLIP: Point Cloud Understanding by CLIP,Perception,https://arxiv.org/abs/2112.02413,,,2021/12/04,,Object Detection,72,,,,
,ChatBridge,ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst,"LLM, Open-source",https://arxiv.org/abs/2305.16103,,,2023/05/25,,Open sourced LLM,73,,,,
,Reflexion,Reflexion: Language Agents with Verbal Reinforcement Learning,Robot,https://arxiv.org/abs/2303.11366,,,2023/03/20,,Self-improvement,74,,,,
,Reward Design with Language Models,Reward Design with Language Models,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2303.00001,,,2023/02/27,,Reinforcement Learning,76,,,,
,FLAN,Finetuned Language Models Are Zero-Shot Learners,"Instruction-Turning, LLM, Zero-shot",https://arxiv.org/abs/2109.01652,,,2021/09/03,,,77,,,,
,EAGER,EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2206.09674,,,2022/06/20,,Reinforcement Learning,78,,,,
,RoboGen,RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation,"Data-generation, Robot",https://arxiv.org/abs/2311.01455,,,2023/11/02,,Data generation,79,,,,
,Foundation Models,"Foundation Models in Robotics: Applications, Challenges, and the Future","Foundation, Robot, Survey",https://arxiv.org/abs/2312.07843,,,2023/12/13,,Survey papers,80,,,,
,GATO,A Generalist Agent,"Agent, Multimodal, Robot",https://arxiv.org/abs/2205.06175,,,2022/05/12,,Multimodal LLM,81,,,,
,ARB,ARB: Advanced Reasoning Benchmark for Large Language Models,"Benchmark, In-Context-Learning",https://arxiv.org/abs/2307.13692,,,2023/07/25,,Benchmark,82,,,,
,Language Models as Zero-Shot Planners,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Robot, Task-Decompose, Zero-shot",https://arxiv.org/abs/2201.07207,,,2022/01/18,,Decomposing task,83,,,,
,Grounding DINO,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,Perception,https://arxiv.org/abs/2303.05499,,,2023/03/09,,Object Detection,84,,,,
,LLM+P,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Agent,https://arxiv.org/abs/2304.11477,,,2023/04/22,,Planning,85,,,,
,Reasoning via Planning,Reasoning with Language Model is Planning with World Model,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2305.14992,,,2023/05/24,,Reasoning,86,,,,
,SayPlan,SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Robot, Task-Decompose",https://arxiv.org/abs/2307.06135,,,2023/07/12,,Decomposing task,87,,,,
,PlanBench,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Benchmark, In-Context-Learning",https://arxiv.org/abs/2206.10498,,,2022/06/21,,Benchmark,88,,,,
,GPT3,Language Models are Few-Shot Learners,LLM,https://arxiv.org/abs/2005.14165,,,2020/05/28,,Closed sourced LLM,89,,,,
,GPT4,GPT-4 Technical Report,"GPT4, LLM",https://arxiv.org/abs/2303.08774,,,2023/03/15,,Closed sourced LLM,91,,,,
80,LLaMA,LLaMA: Open and Efficient Foundation Language Models,"Foundation, LLM, Open-source",https://arxiv.org/abs/2302.13971,,,2023/02/27,,Open sourced LLM,92,,,,
,InstructBLIP,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,"LLM, Open-source",https://arxiv.org/abs/2305.06500,,,2023/05/11,,Open sourced LLM,93,,,,
,Language-conditioned,Language-conditioned Learning for Robotic Manipulation: A Survey,"Robot, Survey",https://arxiv.org/abs/2312.10807,,,2023/12/17,,Survey papers,94,,,,
,SayTap,SayTap: Language to Quadrupedal Locomotion,"Low-level-action, Robot",https://arxiv.org/abs/2306.07580,,,2023/06/13,,Low-level output,96,,,,
,ReAct,ReAct: Synergizing Reasoning and Acting in Language Models,In-Context-Learning,https://arxiv.org/abs/2303.11366,,,2023/03/20,,Reasoning,97,,,,
,Chain-of-Thought Hub,Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.17306,,,2023/05/26,,Benchmark,98,,,,
,MemoryBank,MemoryBank: Enhancing Large Language Models with Long-Term Memory,"LLM, Memory",https://arxiv.org/abs/2305.10250,,,2023/05/17,,Memory,101,,,,
,ChatEval,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,In-Context-Learning,https://arxiv.org/abs/2308.07201,,,2023/08/14,,Memory,102,,,,
,Self-Instruct,Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, LLM",https://arxiv.org/abs/2212.10560,,,2022/12/20,,,103,,,,
,Language Instructed Reinforcement Learning,Language Instructed Reinforcement Learning for Human-AI Coordination,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2304.07297,,,2023/04/13,,Reinforcement Learning,104,,,,
,GLIP,Grounded Language-Image Pre-training,Perception,https://arxiv.org/abs/2112.03857,,,2021/12/07,,Object Detection,106,,,,
,Robotic Brain,LLM as A Robotic Brain: Unifying Egocentric Memory and Control,"Memory, Robot",https://arxiv.org/abs/2304.09349,,,2023/04/19,,Brain,107,,,,
,LLaMA-adapter,LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,"Instruction-Turning, LLM, PEFT",https://arxiv.org/abs/2303.16199,,,2023/03/28,,,108,,,,
,InstructGPT,Training language models to follow instructions with human feedback,"Instruction-Turning, LLM",https://arxiv.org/abs/2203.02155,,,2022/03/04,,,109,,,,
,Rethinking with Retrieval,Rethinking with Retrieval: Faithful Large Language Model Inference,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2301.00303,,,2022/12/31,,Chain of Thought,110,,,,
,EmbodiedGPT,EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,"Chain-of-Thought, Embodied, PersonalCitation, Robot, Task-Decompose",https://arxiv.org/abs/2305.15021,,,2023/05/24,,Chain of Thought,111,,,,
,Skeleton-of-Thought,Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2307.15337,,,2023/07/28,,Chain of Thought,112,,,,
,Self-Consistency,Self-Consistency Improves Chain of Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2203.11171,,,2022/03/21,,Reasoning,113,,,,
,APE,Large Language Models Are Human-Level Prompt Engineers,"Automate, Prompting",https://arxiv.org/abs/2211.01910,,,2022/11/03,,Automation,114,,,,
,LiDAR-LLM,LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding,"Perception, Robot",https://arxiv.org/abs/2312.14074,,,2023/12/21,,Multimodal Data injection,115,,,,
,Multimodal-CoT,Multimodal Chain-of-Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2302.00923,,,2023/02/02,,Chain of Thought,116,,,,
,SayCan,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","LLM, Robot, Task-Decompose",https://arxiv.org/abs/2204.01691,,,2022/04/04,,Decomposing task,117,,,,
,,Text2Motion: From Natural Language Instructions to Feasible Plans,"PersonalCitation, Robot",https://arxiv.org/abs/2303.12153,,,,,,118,,,,
,,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,"Chain-of-Thought, Reasoning, Survey",https://arxiv.org/abs/2212.10001,,,2023/12/20,,,120,,,,
,,Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2401.04398,,,,,,121,,,,
,,GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation,"3D, GPT4, VLM",https://arxiv.org/abs/2401.04092,,,,,,122,,,,
,,Large Language Models are few(1)-shot Table Reasoners,"Reasoning, Table",https://arxiv.org/abs/2210.06710,,,,,,123,,,,
,,Reasoning with Language Model Prompting: A Survey,"Reasoning, Survey",https://arxiv.org/abs/2212.09597,,,,,,124,,,,
,,Learning to Compress Prompts with Gist Tokens,"Compress, Prompting",https://arxiv.org/abs/2304.08467,,,,,,125,,,,
,,Generative Agents: Interactive Simulacra of Human Behavior,Agent,https://arxiv.org/abs/2304.03442,,,,,,126,,,,
,,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,"Agent, Embodied, Robot",https://arxiv.org/abs/2401.12963,,,,,,127,,,,
,,Look Before You Leap: Unveiling the Power ofGPT-4V in Robotic Vision-Language Planning,"Chain-of-Thought, GPT4, Reasoning, Robot",https://robot-vila.github.io/ViLa.pdf,,,2023/11/29,GPT4V,,128,,,,
,,Large Language Models as Generalizable Policies for Embodied Tasks,"Embodied, Robot",,,,,,,129,,,,
,,Interactive Language: Talking to Robots in Real Time,Robot,,,,,,,130,,,,
,Code LLaMA,Code Llama: Open Foundation Models for Code,"Foundation, LLM, Open-source",,,,,,,131,,,,
,ReConcile,ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.,Reasoning,,,,,,,132,,,,
,Selection-Inference,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Reasoning,https://arxiv.org/abs/2205.09712,,,,,,140,,,,
,llama-gpt,"A self-hosted, offline, ChatGPT-like chatbot, powered by Llama 2. 100% private, with no data leaving your device.","LLM, Open-source",,https://github.com/getumbrel/llama-gpt,,,,,141,,,,
,MetaGPT,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,"Agent, Soft-Dev",,,,,,,142,,,,
,ChatDev,Communicative Agents for Software Development,"Agent, Soft-Dev",,https://github.com/OpenBMB/ChatDev,,,,,143,,,,
,XAgent,XAgent: An Autonomous Agent for Complex Task Solving,Agent,https://blog.x-agent.net/blog/xagent/,,,,,,144,,,,
,babyagi,,Agent,,https://github.com/yoheinakajima/babyagi,,,,,145,,,,
,CAMEL,CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society,Agent,,,,,,,146,,,,
,AutoGen,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,Agent,,,,,,,147,,,,
,DSPy,DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,Agent,https://arxiv.org/abs/2310.03714,,,,,,148,,,,
,AutoAgents,AutoAgents: A Framework for Automatic Agent Generation,Agent,,https://github.com/Link-AGI/AutoAgents,,,,,149,,,,
,OpenAgents,OpenAgents: An Open Platform for Language Agents in the Wild,"Agent, Embodied",https://arxiv.org/abs/2310.10634,https://github.com/xlang-ai/OpenAgents,,,,,150,,,,
,Agents,Agents: An Open-source Framework for Autonomous Language Agents,Agent,https://arxiv.org/abs/2309.07870,https://github.com/aiwaves-cn/agents,,,,,151,,,,
,AgentVerse,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,Agent,https://arxiv.org/abs/2308.10848,,,,,,152,,,,
,,AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"In-Context-Learning, Reinforcement-Learning",,,,,,,153,,,,
,,Semantic HELM: A Human-Readable Memory for Reinforcement Learning,"Memory, Reinforcement-Learning",,,,,,,154,,,,
,RLang,RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents,Reinforcement-Learning,,,,,,,155,,,,
,,Large Language Models Are Semi-Parametric Reinforcement Learning Agents,Reinforcement-Learning,,,,,,,156,,,,
,,Instruction-tuning Aligns LLMs to the Human Brain,"Brain, Instruction-Turning",,,,,,,157,,,,
,,LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model,Brain,,,,,,,158,,,,
70,,Divergences between Language Models and Human Brains,"AGI, Brain",,,,,,,159,,,,
,,LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination,Agent,,,,,,,160,,,,
,CogAgent,CogAgent: A Visual Language Model for GUI Agents,"Agent, GUI",,,,,,,161,,,,
,RoCo,RoCo: Dialectic Multi-Robot Collaboration with Large Language Models,Robot,https://arxiv.org/abs/2307.04738,,25,,,,162,,,,
,XAgent,XAgent: An Autonomous Agent for Complex Task Solving,Agent,,,,,,,164,,,,
,,An Interactive Agent Foundation Model,"Agent, End2End, Game, Robot",https://arxiv.org/abs/2402.05929,,,,,,165,,,,
,,Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Benchmark, GPT4",,,,,,,167,,,,
,,When Brain-inspired AI Meets AGI,"AGI, Brain",,,,,,,168,,,,
,,Predictive Minds: LLMs As Atypical Active Inference Agents,Agent,,,,,,,169,,,,
,InfiAgent,InfiAgent: A Multi-Tool Agent for AI Operating Systems,Agent,,,,,,,170,,,,
,NavGPT,NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,"Navigation, Reasoning, Vision",,,,,,,171,,,,
,,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,World-model,,,,,,,172,,,,
,,STARLING: SELF-SUPERVISED TRAINING OF TEXTBASED REINFORCEMENT LEARNING AGENT WITH LARGE LANGUAGE MODELS,"Agent, Reinforcement-Learning",,,,,,,173,,,,
,,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, Embodied",,,,,,,174,,,,
,MindAgent,MindAgent: Emergent Gaming Interaction,Agent,,,,,,,175,,,,
,DetGPT,DetGPT: Detect What You Need via Reasoning,"Perception, Reasoning",https://arxiv.org/abs/2305.14167,,,,,,176,,,,
,,Embodied Question Answering,Enbodied,https://arxiv.org/abs/1711.11543,,,,,,177,,,,
,AppAgent,AppAgent: Multimodal Agents as Smartphone Users,"Agent, GUI, MobileApp",,,,,,,178,,,,
,,Awesome-Multimodal-Large-Language-Models,"Awesome Repo, Multimodal",,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models,,,,,179,,,,
90,,Awesome-Embodied-Agent-with-LLMs,"Agent, Awesome Repo, LLM",,https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs,,,,,180,,,,
,,Awesome-LLM-Robotics,"Awesome Repo, Robot",,https://github.com/GT-RIPL/Awesome-LLM-Robotics,,,,,181,,,,
,,Awesome LLM Reasoning,"Awesome Repo, Reasoning",,https://github.com/atfortes/Awesome-LLM-Reasoning,,,,,182,,,,
,,LLMSurvey,"Awesome Repo, Survey",,https://github.com/RUCAIBox/LLMSurvey,,,,,183,,,,
,,Awesome-Multimodal-LLM,"Awesome Repo, Multimodal",,https://github.com/Atomic-man007/Awesome_Multimodel_LLM,,,,,184,,,,
,,War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,"Agent, Multi",https://arxiv.org/abs/2311.17227,,,,,,185,,,,
50,,Everything-LLMs-And-Robotics,"Awesome Repo, LLM, Robot",,https://github.com/jrin771/Everything-LLMs-And-Robotics,,,,,186,,,,
,,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,"Grounding, Reinforcement-Learning",,,,,,,187,,,,
,,Awesome-Reasoning-Foundation-Models,"Awesome Repo, Reasoning",,https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models,,,,,188,,,,
,,Awesome-LLM,"Awesome Repo, LLM",,https://github.com/Hannibal046/Awesome-LLM,,,,,189,,,,
,,OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics,Robot,,,,,,,190,,,,
,Mobile-Agent,Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception,"Agent, GUI, MobileApp",,,,,,,191,,,,
,,WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,"Agent, Web",,,,,,,192,,,,
,,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,"Prompting, Survey",,,,,,,193,,,,
,,Chain-of-Thought Reasoning Without Prompting,"Chain-of-Thought, Prompting",,,,,,,194,,,,
,,WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,"Agent, Web",,,,,,,195,,,,
,,Self-Discover: Large Language Models Self-Compose Reasoning Structures,Reasoning,,,,,,,196,,,,
,LARP,LARP: Language-Agent Role Play for Open-World Games,"Agent, Minecraft",,,,,,,197,,,,
,,Contrastive Chain-of-Thought Prompting,Prompting,,,,,,,198,,,,
,,Levels of AGI: Operationalizing Progress on the Path to AGI,"AGI, Survey",,,,,,,199,,,,
,,A Survey on LLM-based Autonomous Agents,"Agent, Survey",,https://github.com/Paitesanshi/LLM-Agent-Survey,,,,,200,,,,
,,SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,"Anything, CLIP, Perception",,,,,,,201,,,,
,,Agents: An Open-source Framework for Autonomous Language Agents,Agent,,,,,,,202,,,,
,,"GPT-4V(ision) is a Generalist Web Agent, if Grounded","Agent, GPT4, Web",,,,,,,203,,,,
,,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,"Open-source, Perception",,,,,,,204,,,,
,,DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,Perception,,,,,,,205,,,,
,,World Model on Million-Length Video And Language With RingAttention,"Text-to-Image, World-model",,,,,,,206,,,,
,,日本語LLMまとめ,"Awesome Repo, Japanese, LLM",,https://github.com/llm-jp/awesome-japanese-llm,,,,,207,,,,
,,LLM-Leaderboard,"Awesome Repo, LLM, Leaderboard",,https://github.com/LudwigStumpp/llm-leaderboard,,,,,208,,,,
,,Chain-of-ThoughtsPapers,"Awesome Repo, Chain-of-Thought",,https://github.com/Timothyxxx/Chain-of-ThoughtsPapers,,,,,209,,,,
,,Awesome RLHF (RL with Human Feedback),"Awesome Repo, RLHF, Reinforcement-Learning",,https://github.com/opendilab/awesome-RLHF,,,,,210,,,,
,,LLM-in-Vision,"Awesome Repo, LLM, Vision",,https://github.com/DirtyHarryLYL/LLM-in-Vision,,,,,211,,,,
,,CoALA: Awesome Language Agents,"Agent, Awesome Repo, LLM",,https://github.com/ysymyth/awesome-language-agents,,,,,212,,,,
,,Awesome AI Agents,"Agent, Awesome Repo",,https://github.com/e2b-dev/awesome-ai-agents,,,,,213,,,,
,BC-Z,BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning,"Robot, Zero-shot",https://arxiv.org/abs/2202.02005,,,,,,214,,,,
,,Reasoning Grasping via Multimodal Large Language Model,"Perception, Reasoning, Robot",https://arxiv.org/abs/2402.06798,,,,,,215,,,,
,,Segment Anything,"Anything, Perception, Segmentation",https://arxiv.org/abs/2304.02643,,,,,,216,,,,
,NLaP,Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs,"Embodied, Reasoning, Robot",https://arxiv.org/abs/2403.13801,https://github.com/shure-dev/NLaP,,2024/03/20,,,217,,,,
,,Awesome-LLM-Papers-Toward-AGI,"AGI, Awesome Repo, Survey",,https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI,,,,,218,,,,
70,,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,"LLM, Quantization",https://arxiv.org/abs/2402.17764,,,,,,219,,,,
,,BitNet: Scaling 1-bit Transformers for Large Language Models,"LLM, Scaling",https://arxiv.org/abs/2310.11453,,,,,,220,,,,
,,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"RAG, Temporal Logics",https://arxiv.org/abs/2310.03214,,,,,,221,,,,
,,Large Language Models for Information Retrieval: A Survey,"RAG, Survey",,,,,,,222,,,,
,,TinyLLaVA: A Framework of Small-scale Large Multimodal Models,"LLaVA, VLM",https://arxiv.org/abs/2402.14289,,,,,,223,,,,
,,Recognize Anything: A Strong Image Tagging Model,Perception,https://arxiv.org/abs/2306.03514,,,,,,224,,,,
,OmniACT,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,"Agent, Web",https://arxiv.org/abs/2402.17553,,,,,,225,,,,
,,Video as the New Language for Real-World Decision Making,"Agent, Video-for-Agent",,,,,,,226,,,,
,,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, LLM, Planning",,,,,,,227,,,,
50,,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,"Context-Window, LLM, RoPE, Scaling",https://arxiv.org/abs/2402.13753,,,,,,228,,,,
,,swarms,Agent,,https://github.com/kyegomez/swarms,,,,,229,,,,
,,Gemma: Introducing new state-of-the-art open models,Open-source,https://blog.google/technology/developers/gemma-open-models/,,,,,,230,,,,
,,Learning to Learn Faster from Human Feedback with Language Model Predictive Control,"Feedback, Robot",,,,,,,231,,,,
,,Awesome-Diffusion-Models,"Awesome Repo, Diffusion",,https://github.com/diff-usion/Awesome-Diffusion-Models,,,,,232,,,,
,,Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots,"Robot, Zero-shot",,,,,,,233,,,,
,,Chain-of-Thought Reasoning Without Prompting,Reasoning,https://arxiv.org/abs/2402.10200,,,,,,234,,,,
,,MM-LLMs: Recent Advances in MultiModal Large Language Models,"Survey, VLM",,,,,,,235,,,,
,,ReFT: Reasoning with Reinforced Fine-Tuning,"Reasoning, Reinforcement-Learning",,,,,,,237,,,,
,,Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data,"Anything, Depth",,,,,,,238,,,,
,,"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","Diffusion, Text-to-Image",,,,,,,239,,,,
,,SliceGPT: Compress Large Language Models by Deleting Rows and Columns,"Quantization, Scaling",,,,,,,240,,,,
,,Generative Expressive Robot Behaviors using Large Language Models,Robot,,,,,,,241,,,,
,,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","Awesome Repo, Hallucination, Survey",https://arxiv.org/abs/2311.05232,https://github.com/LuckyyySTA/Awesome-LLM-hallucination,,,,,242,,,,
,,Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,"Context-Window, Scaling",,,,,,,243,,,,
,,Advances in 3D Generation: A Survey,"Generation, Survey",,,,,,,244,,,,
,,A Survey on Evaluation of Large Language Models,"Evaluation, LLM, Survey",https://arxiv.org/abs/2307.03109,,,,,,245,,,,
,,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Math, Reasoning",,,,,,,246,,,,
,,Contrastive Chain-of-Thought Prompting,Reasoning,,,,,,,247,,,,
,,"Retrieval-Augmented Generation for Large Language ","RAG, Survey",,,,,,,248,,,,
,,Rephrase and Respond(RaR),Reasoning,,,,,,,249,,,,
,,Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models,Reasoning,,,,,,,250,,,,
,,OS-Copilot: Towards Generalist Computer Agents with Self-Improvement,"Agent, Web",https://arxiv.org/abs/2402.07456,,,,,,251,,,,
,,Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models,"Generation, Robot, Zero-shot",https://arxiv.org/abs/2310.10639,,,,,,252,,,,
,,Towards Generalizable Zero-Shot Manipulationvia Translating Human Interaction Plans,"Generation, Robot, Zero-shot",,,,,,,253,,,,
80,,Language Models as Zero-Shot Trajectory Generators,"LLM, PersonalCitation, Robot, Zero-shot",https://arxiv.org/abs/2310.11604,,,,,,254,,,,
,,Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting,"Robot, Zero-shot",,,,,,,255,,,,
,,Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,"Robot, Zero-shot",,,,,,,256,,,,
,,Large Language Models are Zero-Shot Reasoners,"Reasoning, Zero-shot",,,,,,,257,,,,
,,Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?,Zero-shot,,,,,,,258,,,,
,,"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models","Sora, Text-to-Video",,,,,,,261,,,,
,,MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,"LLaMA, Lightweight, Open-source",,,,,,,262,,,,
,,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes01,Distilling,,,,,,,263,,,,
,,LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models01,"Benchmark, Sora, Text-to-Video",,,,,,,265,,,,
,,EgoCOT: Embodied Chain-of-Thought Dataset for Vision Language Pre-training,"Chain-of-Thought, Embodied, Robot",,,,,,,266,,,,
,,Language Segment-Anything,"Perception, Robot",,,,,,,267,,,,
,,Large World Model,"VLM, World-model",https://arxiv.org/abs/2402.08268,,,,,,268,,,,
,,Learning and Leveraging World Models in Visual Representation Learning,World-model,,,,,,,269,,,,
,,Resonance RoPE: Improving Context Length Generalization of Large Language Models,"Context-Window, Reasoning, RoPE, Scaling",,,,,,,270,,,,
,,RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation,Robot,,,,,,,271,,,,
,,RoFormer: Enhanced Transformer with Rotary Position Embedding,RoPE,https://arxiv.org/abs/2104.09864,,,,,,272,,,,
,,Language Models Meet World Models: Embodied Experiences Enhance Language Models,"Embodied, World-model",,,,,,,273,,,,
,,[Resource] Paperswithcode,Resource,https://paperswithcode.com/,,,,,,274,,,,
,,[Resource] huggingface,Resource,https://huggingface.co/papers,,,,,,275,,,,
,,Gorilla: Large Language Model Connected with Massive APIs,"Agent, Tool",,,,,,,276,,,,
,,AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,"Reasoning, Robot",,,,,,,277,,,,
,,Introspective Tips: Large Language Model for In-Context Decision Making,Robot,https://www.semanticscholar.org/paper/Introspective-Tips%3A-Large-Language-Model-for-Making-Chen-Wang/047e3812854a86b2a2e113219fa956eda860ce24,,,,,,278,,,,
90,,Multimodal & Large Language Models,"Awesome Repo, LLM, VLM",,https://github.com/Yangyi-Chen/Multimodal-AND-Large-Language-Models,,,,,279,,,,
,,A Survey of Reinforcement Learning from Human Feedback,"RLHF, Reinforcement-Learning, Survey",,,,,,,280,,,,
,,Instruction Tuning for Large Language Models: A Survey,"Instruction-Turning, LLM, Survey",,,,,,,281,,,,
,,Agent AI: Surveying the Horizons of Multimodal Interaction,"Agent, Survey",,,,,,,282,,,,
,,Large Multimodal Agents: A Survey,"Agent, Survey",,,,,,,283,,,,
,,Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning,"Reasoning, Symbolic",,,,,,,284,,,,
,,[Resource] dailyarxiv,Resource,https://dailyarxiv.com/,,,,,,285,,,,
,,You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, MobileApp",,https://github.com/cooelf/Auto-UI,,,,,287,,,,
,,Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning,"Agent, Reasoning",,,,,,,288,,,,
,,Large Language Models for Robotics: A Survey,"LLM, Robot, Survey",,,,,,,289,,,,
,,Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld,"Agent, Embodied",https://arxiv.org/abs/2311.16714,,,,,,291,,,,
,,Octopus: Embodied Vision-Language Programmer from Environmental Feedback,"Agent, Embodied",,,,,,,292,,,,
,,Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning,"Instruction-Turning, Survey",,,,,,,295,,,,
,,Vision-Language Instruction Tuning: A Review and Analysis,"Instruction-Turning, Survey",,,,,,,296,,,,
,,A Closer Look at the Limitations of Instruction Tuning,Instruction-Turning,,,,,,,297,,,,
,,REVO-LION: EVALUATING AND REFINING VISION LANGUAGE INSTRUCTION TUNING DATASETS,"Datatset, Instruction-Turning",,,,,,,298,,,,
,,INSTRUCTION TUNING WITH GPT-4,"GPT4, Instruction-Turning",https://arxiv.org/abs/2304.03277,,,,,,299,,,,
,,Exploring Format Consistency for Instruction Tuning,Instruction-Turning,,,,,,,300,,,,
,,Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models,"Datatset, Instruction-Turning",,,,,,,301,,,,
,,[Resource] Connectedpapers,Resource,https://www.connectedpapers.com/,,,,,,302,,,,
,,RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents,"Agent, Memory, RAG",https://arxiv.org/abs/2402.03610,,,2024/02/06,,,303,,,,
,,Training Language Models with Memory Augmentation,RAG,,,,,,,304,,,,
,,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,MoE,https://arxiv.org/abs/2101.03961,,,,,,305,,,,
,,Language Models Meet World Models,World-model,https://arxiv.org/abs/2305.10626,,,,,,306,,,,
,,Learning to Model the World with Language,World-model,https://arxiv.org/abs/2308.01399,,,,,,307,,,,
,,Diffusion World Model,World-model,https://arxiv.org/abs/2402.03570,,,,,,308,,,,
,,PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,Robot,https://arxiv.org/abs/2402.07872,,,,,,309,,,,
,,RT-H: Action Hierarchies Using Language,"Natural-Language-as-Polices, Robot",https://arxiv.org/abs/2403.01823,,,,,,311,,,,
,,Application of Pretrained Large Language Models in Embodied Artificial Intelligence,"Agent, Embodied, Survey",https://www.semanticscholar.org/paper/Application-of-Pretrained-Large-Language-Models-in-Kovalev-Panov/04f87baf7d1b3eb303a52a8a66c8189f396dd114,,,,,,312,,,,
,,Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,https://arxiv.org/abs/2205.06230,,,,,,313,,,,
,,Design2Code: How Far Are We From Automating Front-End Engineering?,"Code-LLM, Front-End",,,,,,,314,,,,
,,ScreenAgent: A Vision Language Model-driven Computer Control Agent,Agent,,,,,,,315,,,,
70,,XLang Paper Reading,"Agent, Awesome Repo, Embodied, Grounding",,https://github.com/xlang-ai/xlang-paper-reading,,,,,316,,,,
,,Learning to Model the World with Language,World-model,https://arxiv.org/abs/2308.01399,,,,,,319,,,,
,,NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models,"Diffusion, Speech",,,,,,,320,,,,
,,VisionLLaMA: A Unified LLaMA Interface for Vision Tasks,"Foundation, LLaMA, Vision",,,,,,,321,,,,
,,EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions,"Audio2Video, Diffusion, Generation, Video",,,,,,,322,,,,
,,Secrets of RLHF in Large Language Models Part I: PPO,"PPO, RLHF, Reinforcement-Learning",https://arxiv.org/abs/2402.01030,,,2024/02/01,,,323,,,,
,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"Context-Window, Foundation, Gemini, LLM, Scaling",,,,,,,324,,,,
,,"LONGNET: Scaling Transformers to 1,000,000,000 Tokens",Scaling,https://arxiv.org/abs/2307.02486,,,2023/07/01,,,325,,,,
,,STaR: Bootstrapping Reasoning With Reasoning,Reasoning,https://arxiv.org/abs/2203.14465,,,2022/05/28,,,327,,,,
,,The Impact of Reasoning Step Length on Large Language Models,Reasoning,,,,,,,328,,,,
,,A Closer Look at the Limitations of Instruction Tuning,"Instruction-Turning, Survey",https://arxiv.org/abs/2402.05119,,,,,,329,,,,
,,OpenAGI: When LLM Meets Domain Experts,"AGI, Agent",,,54,,,,330,,,,
,,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",Agent,,,,,,,331,,,,
,,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,Instruction-Turning,https://arxiv.org/abs/2302.03202,,27,2023/02/06,,,332,,,,
,,In-Context Instruction Learning,"In-Context-Learning, Instruction-Turning",,,,,,,333,,,,
,,Tuna: Instruction Tuning using Feedback from Large Language Models,Instruction-Turning,https://arxiv.org/abs/2310.13385,,,2023/03/06,,,334,,,,
,,"Structured Prompting: Scaling In-Context Learning to 1,000 Examples","In-Context-Learning, Scaling",,,20,2020/03/06,,,335,,,,
,,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,"In-Context-Learning, Scaling",,,18,2022/03/06,,,336,,,,
,,Secrets of RLHF in Large Language Models Part II: Reward Modeling,RLHF,,,,,,,337,,,,
,,CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval,"Perception, Video, Vision",https://arxiv.org/abs/2104.08860,,,,,,338,,,,
,,LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models,"Image, LLaMA, Perception",,,,,,,339,,,,
,,SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,Perception,,,,,,,340,,,,
,,Visual In-Context Prompting,"In-Context-Learning, Perception, Vision",,,,,,,341,,,,
,,What does CLIP know about a red circle? Visual prompt engineering for VLMs,In-Context-Learning,,,,,,,342,,,,
,,What Makes Good Examples for Visual In-Context Learning?,"In-Context-Learning, Vision",,,,,,,343,,,,
,,Visual Prompting via Image Inpainting,"In-Context-Learning, Vision",,,,,,,344,,,,
,,Prompting Visual-Language Models for Efficient Video Understanding,"In-Context-Learning, Video",,,,,,,345,,,,
,,Visual Prompt Tuning,"In-Context-Learning, Prompt-Tuning",,,,,,,346,,,,
,,Visually Grounded Reasoning across Languages and Cultures,"Grounding, Reasoning",,,,,,,347,,,,
,,V-IRL: Grounding Virtual Intelligence in Real Life,Grounding,,,,,,,348,,,,
,,GLaMM: Pixel Grounding Large Multimodal Model,Grounding,,,,,,,349,,,,
,,Awesome Embodied Vision,"Awesome Repo, Embodied",,https://github.com/ChanganVR/awesome-embodied-vision,,,,,351,,,,
,,PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,Agent,,,,,,,352,,,,
,,Autonomous Agents,"Agent, Awesome Repo",,https://github.com/tmgthb/Autonomous-Agents,,,,,353,,,,
,,3D Diffusion Policy,"Diffusion, Robot",https://arxiv.org/abs/2403.03954,,,,,,354,,,,
,,Segment and Caption Anything,"Anything, Caption, Perception, Segmentation",https://arxiv.org/abs/2312.00869,,,,,,355,,,,
,,Correcting Robot Plans with Natural Language Feedback,"Feedback, Robot",https://arxiv.org/abs/2204.05186,,,,,,356,,,,
,,Real-World Robot Applications of Foundation Models: A Review,"Robot, Survey",,,,,,,357,,,,
,,"Tencent AI Lab - AppAgent, WebVoyager",Lab,,,,,,,362,,,,
,,DeepWisdom - MetaGPT,Lab,,,,,,,363,,,,
,,Reworkd AI - AgentGPT,Lab,,,,,,,364,,,,
,,"OpenBMB - ChatDev, XAgent, AgentVerse",Lab,,,,,,,365,,,,
,,XLANG NLP Lab - OpenAgents,Lab,,,,,,,366,,,,
,,Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,"Chain-of-Thought, Reasoning",,,,,,,367,,,,
,,"Rutgers University, AGI Research - OpenAGI",Lab,,,,,,,368,,,,
,,Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University -  CogVLM,Lab,,,,,,,369,,,,
,,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"Agent, Minecraft",,,,,,,370,,,,
,,You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, GUI, MobileApp",,,,,,,371,,,,
,,"LEARNING EMBODIED VISION-LANGUAGE PRO- GRAMMING FROM INSTRUCTION, EXPLORATION, AND ENVIRONMENTAL FEEDBACK","Agent, Game",,,,,,,372,,,,
,,Embodied Task Planning with Large Language Models,"Agent, Embodied",,,,,,,374,,,,
,,Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym,"Gym, PPO, Reinforcement-Learning, Survey",,,,,,,375,,,,
,,AGENT INSTRUCTS LARGE LANGUAGE MODELS TO BE GENERAL ZERO-SHOT REASONERS,"Agent, Reasoning",,,,,,,376,,,,
,,ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs,"Agent, Tool",,,,,,,377,,,,
,,LLM Powered Autonomous Agents,"Agent, Blog",https://lilianweng.github.io/posts/2023-06-23-agent/,,,,,,378,,,,
,,ScreenAgent: A Computer Control Agent Driven by Visual Language Large Model,"Agent, GUI",,https://github.com/niuzaisheng/ScreenAgent,,,,,379,,,,
,,SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,"Agent, GUI",,,,,,,380,,,,
,,"""What’s important here?"": Opportunities and Challenges of Using LLMs in Retrieving Informatio from Web Interfaces","Agent, GUI, Web",,,,,,,381,,,,
,,Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, Self",,,817,,,,382,,,,https://www.semanticscholar.org/paper/Self-Instruct%3A-Aligning-Language-Models-with-Wang-Kordi/e65b346d442e9962a4276dc1c1af2956d9d5f1eb
,,[Resource] Semanticscholar,Resource,https://www.semanticscholar.org,,,,,,383,,,,
,,A Survey on In-context Learning,"In-Context-Learning, Survey",https://arxiv.org/abs/2301.00234,,,,,,384,,,,
,,Combating Misinformation in the Age of LLMs: Opportunities and Challenges,"Hallucination, Survey",https://arxiv.org/abs/2311.05656,,,,,,385,,,,
,,Awesome-Papers-Autonomous-Agent,"Agent, Awesome Repo",,https://gh.mlsub.net/lafmdp/Awesome-Papers-Autonomous-Agent,,,,,386,,,,
,,Understanding LLMs: A Comprehensive Overview from Training to Inference,"Survey, Training",,,3,,,,387,,,,
,,A Survey on Data Selection for LLM Instruction Tuning,"Instruction-Turning, Survey",,,,,,,388,,,,
,,"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","Agent, Code-LLM, Code-as-Policies, Survey",https://arxiv.org/abs/2401.00812,,5,,,,389,,,,
,,A Survey on Knowledge Distillation of Large Language Models,"Distilling, Survey",,,,,,,390,,,,
,,A Survey on Data Selection for Language Models,"Datatset, LLM, Survey",,,,,,,391,,,,
100,,Awesome-LLM-Survey,"Awesome Repo, LLM, Survey",,https://github.com/HqWu-HITCS/Awesome-LLM-Survey,,,,,392,,,,
70,,Awesome Large Multimodal Agents,"Agent, Awesome Repo",,https://github.com/jun0wanan/awesome-large-multimodal-agents,,,,,393,,,,
,,Awesome Vision-Language Navigation,"Awesome Repo, Perception, VLM",,https://github.com/daqingliu/awesome-vln,,,,,394,,,,
,,Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook,"Survey, TimeSeries",,,,,,,395,,,,
,,OpenGVLab,Lab,,https://github.com/OpenGVLab,,,,,396,,,,
,,Imperial College London - Zeroshot trajectory,Lab,,,,,,,397,,,,
,,[Resource] AlphaSignal,Resource,https://alphasignal.ai/,,,,,,399,,,,
,,[Resource] arxiv-sanity,Resource,https://arxiv-sanity-lite.com/,,,,,,400,,,,
,,InCoRo: In-Context Learning for Robotics Control with Feedback Loops,"Feedback, In-Context-Learning, Robot",,,,,,,403,,,,
,,S-Agents: Self-organizing Agents in Open-ended Environment,"Agent, Minecraft",,,,,,,404,,,,
,,OCI-Robotics: Object-Centric Instruction Augmentation for Robotic Manipulation,Robot,https://arxiv.org/abs/2401.02814,,,,,,406,,,,
,,Lenna: Language Enhanced Reasoning Detection Assistant,"Perception, Reasoning",https://arxiv.org/abs/2312.02433,,,,,,407,,,,
,,Could a Large Language Model be Conscious?,"Brain, Conscious",,,,,,,408,,,,
,,Could a Large Language Model be Conscious?,"Brain, Conscious",,,,,,,409,,,,
,,A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization,Brain,,,,,,,410,,,,
,,(Long)LLMLingua: Enhancing Large Language Model Inference via Prompt Compression,"Compress, Scaling",https://arxiv.org/abs/2310.05736,,,,,,411,,,,
,,LlamaIndex,Package,,https://github.com/run-llama/llama_index,,,,,412,,,,
,,LangChain,Package,,https://github.com/langchain-ai/langchain,,,,,413,,,,
,,h2oGPT,Package,,https://github.com/h2oai/h2ogpt,,,,,414,,,,
60,,Awesome LLMOps,"Awesome Repo, Package",,https://github.com/tensorchord/Awesome-LLMOps,,,,,415,,,,
,,Dify,Package,,https://github.com/langgenius/dify,,,,,416,,,,
,,Alpaca-LoRA,Package,,https://github.com/tloen/alpaca-lora,,,,,417,,,,
,,LoRA: Low-Rank Adaptation of Large Language Models,"LoRA, Scaling",,,,,,,418,,,,
,,Efficient Large Language Models: A Survey,Survey,https://arxiv.org/abs/2312.03863,https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey,,,,,419,,,,
,,Robust Speech Recognition via Large-Scale Weak Supervision,Audio,,,,,,,420,,,,
,,A latent text-to-image diffusion model,Diffusion,,,,,,,421,,,,
,,A Survey on Model Compression for Large Language Models,"Compress, Quantization, Survey",https://arxiv.org/abs/2308.07633,,,,,,422,,,,
30,,Awesome LLM Compression,"Awesome Repo, Compress",,https://github.com/HuangOwen/Awesome-LLM-Compression,,,,Awesome LLM Compression,424,,,,
,40,Cognitive Architectures for Language Agents,Agent,https://arxiv.org/abs/2309.02427,,,,,,426,,,,
,,LLM Agents Papers,"Agent, Awesome Repo",,https://github.com/zjunlp/LLMAgentPapers,,,,,427,,,,
,,Awesome LLM-Powered Agent,"Agent, Awesome Repo",,https://github.com/hyp1231/awesome-llm-powered-agent,,,,,428,,,,
,,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,"LoRA, Scaling",https://arxiv.org/abs/2305.03695,,,,,,429,,,,
,,Video Understanding with Large Language Models: A Survey,"Survey, Video",,,,,,,431,,,,
,,A Survey on Multimodal Large Language Models for Autonomous Driving,"Drive, Survey",https://arxiv.org/abs/2311.12320,,,,,,432,,,,
,,Retrieval-Augmented Generation for Large Language Models: A Survey,"RAG, Survey",,,,,,,433,,,,
,,On the Design Fundamentals of Diffusion Models: A Survey,"Diffusion, Survey",https://arxiv.org/abs/2306.04542,,,,,,434,,,,
,,Chain-of-table: Evolving tables in the reasoning chain for table understanding,"Chain-of-Thought, Reasoning, Table",,,,,,,435,,,,
,,DeepSeek-VL: Towards Real-World Vision-Language Understanding01,"VLM, VQA",,,,,,,436,,,,
,,Tree-Planner: Efficient Close-loop Task Planning with Large Language Models01,"LLM, PersonalCitation, Robot",,,,,,,437,,,,
,,GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration,"Demonstration, GPT4, PersonalCitation, Robot",,,,,,,438,,,,
,,IROS2023PaperList,"Awesome Repo, IROS, Robot",,https://github.com/gonultasbu/IROS2023PaperList,,,,,439,,,,
,,Paper List for In-context Learning,"Awesome Repo, In-Context-Learning",,https://github.com/dqxiu/ICL_PaperList,,,,,440,,,,
,,OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,"Agent, Embodied, Robot",,,,,,,441,,,,
,,Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Chain-of-Thought, Code-as-Policies",https://arxiv.org/abs/2312.04474,,,,,,442,,,,
,,AgentTuning: Enabling Generalized Agent Abilities For LLMs,"Agent, Instruction-Turning",https://arxiv.org/abs/2310.12823,,,,,Comprehensive Collection of LLM-Related Papers,443,,,,
,,Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory01,"Agent, Minecraft",,,,,,,444,,,,
,,ScreenAI: A Vision-Language Model for UI and Infographics Understanding,VLM,,,,,,,445,,,,
,,Prompt a Robot to Walk with Large Language Models,"Action-Generation, Generation, Prompting",,,,,,,446,,,,
,,Can large language models explore in-context?,In-Context-Learning,,,,,,,448,,,,
,,RoFormer: Enhanced Transformer with Rotary Position Embedding,Context-Window,,,,,,,449,,,,
,,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,"Context-Window, Foundation",,,,,,,451,,,,
,,Corrective Retrieval Augmented Generation,"CRAG, RAG",https://arxiv.org/abs/2401.15884,,,,,,452,,,,
,,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",RAG,,,,,,,453,,,,
,,RAG-Fusion: a New Take on Retrieval-Augmented Generation,RAG,https://arxiv.org/abs/2402.03367,,,,,,454,,,,
,,Gorilla: Large Language Model Connected with Massive APIs,"APIs, Agent, Tool",https://arxiv.org/abs/2305.15334,,,,,,455,,,,
,,RAFT: Adapting Language Model to Domain Specific RAG,RAG,https://arxiv.org/abs/2403.10131,,,,,,456,,,,
,,AIOS: LLM Agent Operating System,Agent,https://arxiv.org/abs/2403.16971,,,,,,457,,,,
,,"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",Agent,https://arxiv.org/abs/2312.03815,,,,,,458,,,,
,,Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,RAG,https://arxiv.org/abs/2403.14403,,,,,,459,,,,
,,Explorative Inbetweening of Time and Space,Temporal,https://arxiv.org/abs/2403.14611,,,,,,460,,,,
,,"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies",Robot,https://arxiv.org/abs/2403.07832,,,,,,461,,,,
,,LaVague,"Action-Model, Agent, LAM",,https://github.com/lavague-ai/LaVague,,,,,462,,,,
,,Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation,Tex2Img,https://arxiv.org/abs/2403.16990,,,,,,463,,,,
,,Awesome-Chinese-LLM,"Awesome Repo, Chinese",,https://github.com/HqWu-HITCS/Awesome-Chinese-LLM,,,,,464,,,,
,,awesome-korean-llm,"Awesome Repo, Korean",,https://github.com/NomaDamas/awesome-korean-llm,,,,,465,,,,
,,InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding,"ViFM, Video",https://arxiv.org/abs/2403.15377,https://github.com/OpenGVLab/InternVideo2/,,,,,466,,,,
,,Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,"Sora, Text-to-Video",https://arxiv.org/abs/2403.13248,,,,,,468,,,,
,,open-interpreter,"Agent-Project, Code-LLM",,https://github.com/OpenInterpreter/open-interpreter,,,,,469,,,,