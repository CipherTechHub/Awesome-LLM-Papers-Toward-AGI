Impact,Model,Title,Category,ArXiv Link,GitHub,Citation,Publication Date,Property
,,Segment and Caption Anything,"Anything, Caption, Perception, Segmentation",https://arxiv.org/abs/2312.00869,,,,
,,open-interpreter,"Agent-Project, Code-LLM",,https://github.com/OpenInterpreter/open-interpreter,,,
,,Explorative Inbetweening of Time and Space,Temporal,https://arxiv.org/abs/2403.14611,,,,
,,"Tencent AI Lab - AppAgent, WebVoyager",Lab,,,,,
,,Correcting Robot Plans with Natural Language Feedback,"Feedback, Robot",https://arxiv.org/abs/2204.05186,,,,
70,,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits,"LLM, Quantization",https://arxiv.org/abs/2402.17764,,,,
,,EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions,"Audio2Video, Diffusion, Generation, Video",,,,,
,,VisionLLaMA: A Unified LLaMA Interface for Vision Tasks,"Foundation, LLaMA, Vision",,,,,
,,NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models,"Diffusion, Speech",,,,,
,,Design2Code: How Far Are We From Automating Front-End Engineering?,"Code-LLM, Front-End",,,,,
,,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,MoE,https://arxiv.org/abs/2101.03961,,,,
,,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes01,Distilling,,,,,
,,SliceGPT: Compress Large Language Models by Deleting Rows and Columns,"Quantization, Scaling",,,,,
,,"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","Diffusion, Text-to-Image",,,,,
,,Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data,"Anything, Depth",,,,,
,,Learning to Learn Faster from Human Feedback with Language Model Predictive Control,"Feedback, Robot",,,,,
50,,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,"Context-Window, LLM, RoPE, Scaling",https://arxiv.org/abs/2402.13753,,,,
,,Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Benchmark, GPT4",,,,,
,BC-Z,BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning,"Robot, Zero-shot",https://arxiv.org/abs/2202.02005,,,,
,,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,"Grounding, Reinforcement-Learning",,,,,
,Code LLaMA,Code Llama: Open Foundation Models for Code,"Foundation, LLM, Open-source",,,,,
,,Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning,World-model,,,,,
,,World Model on Million-Length Video And Language With RingAttention,"Text-to-Image, World-model",,,,,
,,Embodied Question Answering,Enbodied,https://arxiv.org/abs/1711.11543,,,,
,,Awesome-Multimodal-LLM,"Awesome Repo, Multimodal",,https://github.com/Atomic-man007/Awesome_Multimodel_LLM,,,
,,LLMSurvey,"Awesome Repo, Survey",,https://github.com/RUCAIBox/LLMSurvey,,,
,,Awesome LLM Reasoning,"Awesome Repo, Reasoning",,https://github.com/atfortes/Awesome-LLM-Reasoning,,,
,,Awesome-LLM-Robotics,"Awesome Repo, Robot",,https://github.com/GT-RIPL/Awesome-LLM-Robotics,,,
90,,Awesome-Embodied-Agent-with-LLMs,"Agent, Awesome Repo, LLM",,https://github.com/zchoi/Awesome-Embodied-Agent-with-LLMs,,,
,,Awesome-Multimodal-Large-Language-Models,"Awesome Repo, Multimodal",,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models,,,
,,Awesome-LLM-Papers-Toward-AGI,"AGI, Awesome Repo, Survey",,https://github.com/shure-dev/Awesome-LLM-Papers-Toward-AGI,,,
,,Awesome AI Agents,"Agent, Awesome Repo",,https://github.com/e2b-dev/awesome-ai-agents,,,
,,CoALA: Awesome Language Agents,"Agent, Awesome Repo, LLM",,https://github.com/ysymyth/awesome-language-agents,,,
,,LLM-in-Vision,"Awesome Repo, LLM, Vision",,https://github.com/DirtyHarryLYL/LLM-in-Vision,,,
,,Awesome RLHF (RL with Human Feedback),"Awesome Repo, RLHF, Reinforcement-Learning",,https://github.com/opendilab/awesome-RLHF,,,
,,Chain-of-ThoughtsPapers,"Awesome Repo, Chain-of-Thought",,https://github.com/Timothyxxx/Chain-of-ThoughtsPapers,,,
,,LLM-Leaderboard,"Awesome Repo, LLM, Leaderboard",,https://github.com/LudwigStumpp/llm-leaderboard,,,
,,日本語LLMまとめ,"Awesome Repo, Japanese, LLM",,https://github.com/llm-jp/awesome-japanese-llm,,,
,,Awesome-LLM,"Awesome Repo, LLM",,https://github.com/Hannibal046/Awesome-LLM,,,
,,Awesome-Reasoning-Foundation-Models,"Awesome Repo, Reasoning",,https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models,,,
50,,Everything-LLMs-And-Robotics,"Awesome Repo, LLM, Robot",,https://github.com/jrin771/Everything-LLMs-And-Robotics,,,
,,Semantic HELM: A Human-Readable Memory for Reinforcement Learning,"Memory, Reinforcement-Learning",,,,,
,,Large Language Models Are Semi-Parametric Reinforcement Learning Agents,Reinforcement-Learning,,,,,
,RLang,RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents,Reinforcement-Learning,,,,,
,,When Brain-inspired AI Meets AGI,"AGI, Brain",,,,,
,,Instruction-tuning Aligns LLMs to the Human Brain,"Brain, Instruction-Turning",,,,,
70,,Divergences between Language Models and Human Brains,"AGI, Brain",,,,,
,,LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model,Brain,,,,,
20,Gpt-driver,GPT-Driver: Learning to Drive with GPT,"Driving, Spacial",https://arxiv.org/abs/2310.01415,,,2023/10/02,Spatial Understanding
,,GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation,"3D, GPT4, VLM",https://arxiv.org/abs/2401.04092,,,,
,GPT4-V OpenFlamingo,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"Open-source, VLM",https://arxiv.org/abs/2308.01390,,,2023/08/02,Vision-LLM
,InternGPT,InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language,"Intaractive, OpenGVLab, VLM",https://arxiv.org/abs/2305.05662,,,2023/05/09,Vision-LLM
,PaLM,PaLM: Scaling Language Modeling with Pathways,VLM,https://arxiv.org/abs/2204.02311,,,2022/04/05,Vision-LLM
,CogVLM,CogVLM: Visual Expert for Pretrained Language Models,"VLM, VQA",https://arxiv.org/abs/2311.03079,,,2023/11/06,Visual Question Answering
,ViperGPT,ViperGPT: Visual Inference via Python Execution for Reasoning,"Code-as-Policies, Reasoning, VLM, VQA",https://arxiv.org/abs/2303.08128,,,2023/03/14,Visual Question Answering
,VISPROG,Visual Programming: Compositional visual reasoning without training,"Code-as-Policies, VLM, VQA",https://arxiv.org/abs/2211.11559,,,2022/11/18,Visual Question Answering
,MM-ReAct,MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,"Reasoning, VLM, VQA",https://arxiv.org/abs/2303.11381,,,2023/03/20,Visual Question Answering
,Chameleon,Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models,"VLM, VQA",https://arxiv.org/abs/2304.09842,,,2023/04/19,Visual Question Answering
,Caption Anything,Caption Anything: Interactive Image Description with Diverse Multimodal Controls,"Caption, VLM, VQA",https://arxiv.org/abs/2305.02677,,,2023/05/04,Visual Question Answering
,,Learning to Compress Prompts with Gist Tokens,"Compress, Prompting",https://arxiv.org/abs/2304.08467,,,,
,APE,Large Language Models Are Human-Level Prompt Engineers,"Automate, Prompting",https://arxiv.org/abs/2211.01910,,,2022/11/03,Automation
,,Contrastive Chain-of-Thought Prompting,Prompting,,,,,
,,Chain-of-Thought Reasoning Without Prompting,"Chain-of-Thought, Prompting",,,,,
,NavGPT,NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models,Reasoning,,,,,
,Selection-Inference,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Reasoning,https://arxiv.org/abs/2205.09712,,,,
,ReConcile,ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.,Reasoning,,,,,
,,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,"Chain-of-Thought, Reasoning, Survey",https://arxiv.org/abs/2212.10001,,,2023/12/20,
,,Large Language Models are few(1)-shot Table Reasoners,"Reasoning, Table",https://arxiv.org/abs/2210.06710,,,,
,,Reasoning with Language Model Prompting: A Survey,"Reasoning, Survey",https://arxiv.org/abs/2212.09597,,,,
50,Chain of Thought,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2201.11903,,,2022/01/28,Chain of Thought
,Tree of Thought,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.10601,,,2023/05/17,Chain of Thought
,Multimodal-CoT,Multimodal Chain-of-Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2302.00923,,,2023/02/02,Chain of Thought
,Auto-CoT,Automatic Chain of Thought Prompting in Large Language Models,"Automate, Chain-of-Thought, Reasoning",https://arxiv.org/abs/2210.03493,,,2022/10/07,Chain of Thought
,Verify-and-Edit,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.03268,,,2023/05/05,Chain of Thought
,Skeleton-of-Thought,Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2307.15337,,,2023/07/28,Chain of Thought
,Rethinking with Retrieval,Rethinking with Retrieval: Faithful Large Language Model Inference,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2301.00303,,,2022/12/31,Chain of Thought
,Self-Consistency,Self-Consistency Improves Chain of Thought Reasoning in Language Models,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2203.11171,,,2022/03/21,Reasoning
,SelfCheck,SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning,"Chain-of-Thought, Planning, Reasoning",https://arxiv.org/abs/2308.00436,,,2023/08/01,Planning
,Chain-of-Thought Hub,Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,"Chain-of-Thought, Reasoning",https://arxiv.org/abs/2305.17306,,,2023/05/26,Benchmark
,A Survey of Chain of Thought Reasoning,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","Chain-of-Thought, Reasoning, Survey",https://arxiv.org/abs/2309.15402,,,2023/09/27,Survey Paper
,NLaP,Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs,"Embodied, Reasoning, Robot",https://arxiv.org/abs/2403.13801,https://github.com/shure-dev/NLaP,,2024/03/20,
,,Self-Discover: Large Language Models Self-Compose Reasoning Structures,Reasoning,,,,,
,,A Survey on LLM-based Autonomous Agents,"Agent, Survey",,https://github.com/Paitesanshi/LLM-Agent-Survey,,,
,,Levels of AGI: Operationalizing Progress on the Path to AGI,"AGI, Survey",,,,,
,,A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications,"Prompting, Survey",,,,,
,DetGPT,DetGPT: Detect What You Need via Reasoning,"Perception, Reasoning",https://arxiv.org/abs/2305.14167,,,,
,OWL-ViT,Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,https://arxiv.org/abs/2205.06230,,,2022/05/12,Object Detection
,GLIP,Grounded Language-Image Pre-training,Perception,https://arxiv.org/abs/2112.03857,,,2021/12/07,Object Detection
,Grounding DINO,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,Perception,https://arxiv.org/abs/2303.05499,,,2023/03/09,Object Detection
,PointCLIP,PointCLIP: Point Cloud Understanding by CLIP,Perception,https://arxiv.org/abs/2112.02413,,,2021/12/04,Object Detection
,Segment Anything,Segment Anything,"LLM, Open-source, Perception, Segmentation",https://arxiv.org/abs/2304.02643,,,2023/04/05,Object Detection
,DOREMI,DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment,"Perception, Task-Decompose",https://arxiv.org/abs/2307.00329,,,2023/07/01,Decomposing task
,3D-LLM,3D-LLM: Injecting the 3D World into Large Language Models,"3D, Open-source, Perception, Robot",https://arxiv.org/abs/2307.12981,,,2023/07/24,Multimodal Data injection
,,Segment Anything,"Anything, Perception, Segmentation",https://arxiv.org/abs/2304.02643,,,,
,,Reasoning Grasping via Multimodal Large Language Model,"Perception, Reasoning, Robot",https://arxiv.org/abs/2402.06798,,,,
,,DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,Perception,,,,,
,,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,"Open-source, Perception",,,,,
,,SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,"Anything, CLIP, Perception",,,,,
,,OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics,Robot,,,,,
,RoCo,RoCo: Dialectic Multi-Robot Collaboration with Large Language Models,Robot,https://arxiv.org/abs/2307.04738,,25,,
,,Look Before You Leap: Unveiling the Power ofGPT-4V in Robotic Vision-Language Planning,"Chain-of-Thought, GPT4, Reasoning, Robot",https://robot-vila.github.io/ViLa.pdf,,,2023/11/29,
,,Interactive Language: Talking to Robots in Real Time,Robot,,,,,
,,Large Language Models as Generalizable Policies for Embodied Tasks,"Embodied, Robot",,,,,
,,AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents,"Agent, Embodied, Robot",https://arxiv.org/abs/2401.12963,,,,
,,Inner Monologue: Embodied Reasoning through Planning with Language Models,"Code-as-Policies, Embodied, PersonalCitation, Reasoning, Robot, Task-Decompose",https://arxiv.org/abs/2207.05608,,,,
,,Text2Motion: From Natural Language Instructions to Feasible Plans,"PersonalCitation, Robot",https://arxiv.org/abs/2303.12153,,,,
,VIMA,VIMA: General Robot Manipulation with Multimodal Prompts,"End2End, Multimodal, Robot",https://arxiv.org/abs/2210.03094,,,2022/10/06,Multimodal prompts
,Instruct2Act,Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model,"Code-as-Policies, Multimodal, OpenGVLab, PersonalCitation, Robot",https://arxiv.org/abs/2305.11176,,,2023/05/18,Multimodal prompts
,MOMA-Force,MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation,"Multimodal, Robot",https://arxiv.org/abs/2308.03624,,,2023/08/07,Multimodal prompts
,PaLM-E,PaLM-E: An Embodied Multimodal Language Model,"End2End, Multimodal, Robot",https://arxiv.org/abs/2303.03378,,,2023/03/06,Multimodal LLM
,GATO,A Generalist Agent,"Agent, Multimodal, Robot",https://arxiv.org/abs/2205.06175,,,2022/05/12,Multimodal LLM
,Flamingo,Flamingo: a Visual Language Model for Few-Shot Learning,"Multimodal, Robot",https://arxiv.org/abs/2204.14198,,,2022/04/29,Multimodal LLM
,Physically Grounded Vision-Language Model,Physically Grounded Vision-Language Models for Robotic Manipulation,"End2End, Multimodal, Robot",https://arxiv.org/abs/2309.02561,,,2023/09/05,Multimodal LLM
,MOO,Open-World Object Manipulation using Pre-trained Vision-Language Models,"Multimodal, Robot",https://arxiv.org/abs/2303.00905,,,2023/03/02,Multimodal LLM
,Code as policies,Code as Policies: Language Model Programs for Embodied Control,"Code-as-Policies, Embodied, PersonalCitation, Robot",https://arxiv.org/abs/2209.07753,,,2022/09/16,Code generation
,Progprompt,ProgPrompt: Generating Situated Robot Task Plans using Large Language Models,"Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2209.11302,,,2022/09/22,Code generation
,Socratic,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,"Code-as-Policies, PersonalCitation, Robot, Zero-shot",https://arxiv.org/abs/2204.00598,,,2022/04/01,Code generation
,SMART-LLM,SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,"Code-as-Policies, Robot",https://arxiv.org/abs/2309.10062,,,2023/09/18,Code generation
,Statler,Statler: State-Maintaining Language Models for Embodied Reasoning,"Code-as-Policies, PersonalCitation, Robot, State-Manage",https://arxiv.org/abs/2306.17840,,,2023/06/30,Code generation
,SayCan,"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","LLM, Robot, Task-Decompose",https://arxiv.org/abs/2204.01691,,,2022/04/04,Decomposing task
,Language Models as Zero-Shot Planners,Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents,"Robot, Task-Decompose, Zero-shot",https://arxiv.org/abs/2201.07207,,,2022/01/18,Decomposing task
,SayPlan,SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Robot, Task-Decompose",https://arxiv.org/abs/2307.06135,,,2023/07/12,Decomposing task
,SayTap,SayTap: Language to Quadrupedal Locomotion,"Low-level-action, Robot",https://arxiv.org/abs/2306.07580,,,2023/06/13,Low-level output
,Prompt a Robot to Walk,Prompt a Robot to Walk with Large Language Models,"Low-level-action, Robot",https://arxiv.org/abs/2309.09969,,,2023/09/18,Low-level output
,LiDAR-LLM,LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding,"Perception, Robot",https://arxiv.org/abs/2312.14074,,,2023/12/21,Multimodal Data injection
,Gensim,GenSim: Generating Robotic Simulation Tasks via Large Language Models,"Data-generation, Robot",https://arxiv.org/abs/2310.01361,,,2023/10/02,Data generation
,RoboGen,RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation,"Data-generation, Robot",https://arxiv.org/abs/2311.01455,,,2023/11/02,Data generation
,Embodied Task Planning,Embodied Task Planning with Large Language Models,"Embodied, Robot, Task-Decompose",https://arxiv.org/abs/2307.01848,,,2023/07/04,Planning
,REFLECT,REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction,"Feedback, Robot",https://arxiv.org/abs/2306.15724,,,2023/06/27,Self-improvement
,Reflexion,Reflexion: Language Agents with Verbal Reinforcement Learning,Robot,https://arxiv.org/abs/2303.11366,,,2023/03/20,Self-improvement
,EmbodiedGPT,EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,"Chain-of-Thought, Embodied, PersonalCitation, Robot, Task-Decompose",https://arxiv.org/abs/2305.15021,,,2023/05/24,Chain of Thought
,Robotic Brain,LLM as A Robotic Brain: Unifying Egocentric Memory and Control,"Memory, Robot",https://arxiv.org/abs/2304.09349,,,2023/04/19,Brain
,Toward General-Purpose,Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis,"Robot, Survey",https://arxiv.org/abs/2312.08782,,,2023/12/14,Survey papers
,Language-conditioned,Language-conditioned Learning for Robotic Manipulation: A Survey,"Robot, Survey",https://arxiv.org/abs/2312.10807,,,2023/12/17,Survey papers
,Foundation Models,"Foundation Models in Robotics: Applications, Challenges, and the Future","Foundation, Robot, Survey",https://arxiv.org/abs/2312.07843,,,2023/12/13,Survey papers
,Robot Learning,Robot Learning in the Era of Foundation Models: A Survey,"Robot, Survey",https://arxiv.org/abs/2311.14379,,,2023/11/24,Survey papers
,The Development of LLMs,The Development of LLMs for Embodied Navigation,"Embodied, LLM, Robot, Survey",https://arxiv.org/abs/2311.00530,,,2023/11/01,Survey papers
,,An Interactive Agent Foundation Model,"Agent, End2End, Game, Robot",https://arxiv.org/abs/2402.05929,,,,
,XAgent,XAgent: An Autonomous Agent for Complex Task Solving,Agent,,,,,
,CogAgent,CogAgent: A Visual Language Model for GUI Agents,"Agent, GUI",,,,,
,,LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination,Agent,,,,,
,AgentVerse,AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors,Agent,https://arxiv.org/abs/2308.10848,,,,
,Agents,Agents: An Open-source Framework for Autonomous Language Agents,Agent,https://arxiv.org/abs/2309.07870,https://github.com/aiwaves-cn/agents,,,
,OpenAgents,OpenAgents: An Open Platform for Language Agents in the Wild,"Agent, Embodied",https://arxiv.org/abs/2310.10634,https://github.com/xlang-ai/OpenAgents,,,
,AutoAgents,AutoAgents: A Framework for Automatic Agent Generation,Agent,,https://github.com/Link-AGI/AutoAgents,,,
,DSPy,DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,Agent,https://arxiv.org/abs/2310.03714,,,,
,AutoGen,AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,Agent,,,,,
,CAMEL,CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society,Agent,,,,,
,babyagi,,Agent,,https://github.com/yoheinakajima/babyagi,,,
,XAgent,XAgent: An Autonomous Agent for Complex Task Solving,Agent,https://blog.x-agent.net/blog/xagent/,,,,
,ChatDev,Communicative Agents for Software Development,"Agent, Soft-Dev",,https://github.com/OpenBMB/ChatDev,,,
,MetaGPT,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,"Agent, Soft-Dev",,,,,
,,Generative Agents: Interactive Simulacra of Human Behavior,Agent,https://arxiv.org/abs/2304.03442,,,,
,Voyager,Voyager: An Open-Ended Embodied Agent with Large Language Models,"Agent, Minecraft",https://arxiv.org/abs/2305.16291,,,2023/05/25,Planning
,DEPS,"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","Agent, Minecraft",https://arxiv.org/abs/2302.01560,,,2023/02/03,Planning
,JARVIS-1,JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models,"Agent, Memory, Minecraft",https://arxiv.org/abs/2311.05997,,,2023/11/10,Planning
,LLM+P,LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,Agent,https://arxiv.org/abs/2304.11477,,,2023/04/22,Planning
,Autonomous Agents,A Survey on Large Language Model based Autonomous Agents,"Agent, Survey",https://arxiv.org/abs/2308.11432,,,2023/08/22,Planning
,AgentInstruct,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,"Agent, Reasoning, Zero-shot",https://arxiv.org/abs/2310.03710,,,2023/10/05,Planning
,Eureka,Eureka: Human-Level Reward Design via Coding Large Language Models,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2310.12931,,,2023/10/19,Reinforcement Learning
,Language to Rewards,Language to Rewards for Robotic Skill Synthesis,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2306.08647,,,2023/06/14,Reinforcement Learning
,Language Instructed Reinforcement Learning,Language Instructed Reinforcement Learning for Human-AI Coordination,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2304.07297,,,2023/04/13,Reinforcement Learning
,Lafite-RL,Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models,"Agent, Feedback, Reinforcement-Learning, Robot",https://arxiv.org/abs/2311.02379,,,2023/11/04,Reinforcement Learning
,ELLM,Guiding Pretraining in Reinforcement Learning with Large Language Models,"Agent, Reinforcement-Learning",https://arxiv.org/abs/2302.06692,,,2023/02/13,Reinforcement Learning
,RLAdapter,RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds,"Agent, Minecraft, Reinforcement-Learning",,,,,Reinforcement Learning
,AdaRefiner,AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback,"Agent, Feedback, Reinforcement-Learning",https://arxiv.org/abs/2309.17176,,,2023/09/29,Reinforcement Learning
,Reward Design with Language Models,Reward Design with Language Models,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2303.00001,,,2023/02/27,Reinforcement Learning
,EAGER,EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2206.09674,,,2022/06/20,Reinforcement Learning
,Text2Reward,Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning,"Agent, Reinforcement-Learning, Reward",https://arxiv.org/abs/2309.11489,,,2023/09/20,Reinforcement Learning
,AgentSims,AgentSims: An Open-Source Sandbox for Large Language Model Evaluation,Agent,https://arxiv.org/abs/2308.04026,,,2023/08/08,Open-Source Evaluation
90,Large Language Model Based Agents,The Rise and Potential of Large Language Model Based Agents: A Survey,"Agent, Survey",https://arxiv.org/abs/2309.07864,,,2023/09/14,Survey Paper
,,"GPT-4V(ision) is a Generalist Web Agent, if Grounded","Agent, GPT4, Web",,,,,
,,Agents: An Open-source Framework for Autonomous Language Agents,Agent,,,,,
,LARP,LARP: Language-Agent Role Play for Open-World Games,"Agent, Minecraft",,,,,
,,WebLINX: Real-World Website Navigation with Multi-Turn Dialogue,"Agent, Web",,,,,
,,WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models,"Agent, Web",,,,,
,Mobile-Agent,Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception,"Agent, GUI, MobileApp",,,,,
,,StarCoder 2 and The Stack v2: The Next Generation,Code-LLM,,,,,
,,BitNet: Scaling 1-bit Transformers for Large Language Models,"LLM, Scaling",https://arxiv.org/abs/2310.11453,,,,
,llama-gpt,"A self-hosted, offline, ChatGPT-like chatbot, powered by Llama 2. 100% private, with no data leaving your device.","LLM, Open-source",,https://github.com/getumbrel/llama-gpt,,,
80,LLaMA,LLaMA: Open and Efficient Foundation Language Models,"Foundation, LLM, Open-source",https://arxiv.org/abs/2302.13971,,,2023/02/27,Open sourced LLM
,OpenFlamingo,OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,"LLM, Open-source",https://arxiv.org/abs/2308.01390,,,2023/08/02,Open sourced LLM
,InstructBLIP,InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning,"LLM, Open-source",https://arxiv.org/abs/2305.06500,,,2023/05/11,Open sourced LLM
,ChatBridge,ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst,"LLM, Open-source",https://arxiv.org/abs/2305.16103,,,2023/05/25,Open sourced LLM
,GPT3,Language Models are Few-Shot Learners,LLM,https://arxiv.org/abs/2005.14165,,,2020/05/28,Closed sourced LLM
,GPT4,GPT-4 Technical Report,"GPT4, LLM",https://arxiv.org/abs/2303.08774,,,2023/03/15,Closed sourced LLM
,InstructGPT,Training language models to follow instructions with human feedback,"Instruction-Turning, LLM",https://arxiv.org/abs/2203.02155,,,2022/03/04,
,LLaVA,Visual Instruction Tuning,"Instruction-Turning, LLM, PEFT",https://arxiv.org/abs/2304.08485,,,2023/04/17,
,MiniGPT-4,MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,"Instruction-Turning, LLM",https://arxiv.org/abs/2304.10592,,,2023/04/20,
,FLAN,Finetuned Language Models Are Zero-Shot Learners,"Instruction-Turning, LLM, Zero-shot",https://arxiv.org/abs/2109.01652,,,2021/09/03,
,LLaMA-adapter,LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,"Instruction-Turning, LLM, PEFT",https://arxiv.org/abs/2303.16199,,,2023/03/28,
,Self-Instruct,Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, LLM",https://arxiv.org/abs/2212.10560,,,2022/12/20,
,Path planners,Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning,"LLM, Spacial",https://arxiv.org/abs/2310.03249,,,2023/10/05,Spatial Understanding
,NL2TL,NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models,"LLM, Temporal Logics",https://arxiv.org/abs/2305.07766,,,2023/05/12,Temporal Logics
,GPT4Vis,GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?,"LLM, Zero-shot",https://arxiv.org/abs/2311.15732,,,2023/11/27,Quantitive Analysis
,Gemini vs GPT-4V,Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases,"GPT4, Gemini, LLM",https://arxiv.org/abs/2312.15011,,,2023/12/22,Quantitive Analysis
,A Survey of Large Language Models,A Survey of Large Language Models,"LLM, Survey",https://arxiv.org/abs/2303.18223,,,2023/03/31,Survey Papers
,MemoryBank,MemoryBank: Enhancing Large Language Models with Long-Term Memory,"LLM, Memory",https://arxiv.org/abs/2305.10250,,,2023/05/17,Memory
,Reasoning in Large Language Models,Towards Reasoning in Large Language Models: A Survey,"LLM, Reasoning, Survey",https://arxiv.org/abs/2212.10403,,,2022/12/20,Survey Paper
,,AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,"In-Context-Learning, Reinforcement-Learning",,,,,
,,Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2401.04398,,,,
,ReAct,ReAct: Synergizing Reasoning and Acting in Language Models,In-Context-Learning,https://arxiv.org/abs/2303.11366,,,2023/03/20,Reasoning
,Self-Refine,Self-Refine: Iterative Refinement with Self-Feedback,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2303.17651,,,2023/03/30,Reasoning
,Plan-and-Solve,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2305.04091,,,2023/05/06,Reasoning
,PAL,PAL: Program-aided Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2211.10435,,,2022/11/18,Reasoning
,Reasoning via Planning,Reasoning with Language Model is Planning with World Model,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2305.14992,,,2023/05/24,Reasoning
,Self-Ask,Measuring and Narrowing the Compositionality Gap in Language Models,"Chain-of-Thought, In-Context-Learning, Self",https://arxiv.org/abs/2210.03350,,,2022/10/07,Reasoning
,Least-to-Most Prompting,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2205.10625,,,2022/05/21,Reasoning
,Self-Polish,Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement,"Chain-of-Thought, In-Context-Learning, Self",https://arxiv.org/abs/2305.14497,,,2023/05/23,Reasoning
,COMPLEXITY-CoT,Complexity-Based Prompting for Multi-Step Reasoning,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2210.00720,,,2022/10/03,Reasoning
,Maieutic Prompting,Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2205.11822,,,2022/05/24,Reasoning
,Algorithm of Thoughts,Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models,"Chain-of-Thought, In-Context-Learning",https://arxiv.org/abs/2308.10379,,,2023/08/20,Reasoning
,SuperICL,Small Models are Valuable Plug-ins for Large Language Models,In-Context-Learning,https://arxiv.org/abs/2305.08848,,,2023/05/15,Reasoning
,VisualCOMET,VisualCOMET: Reasoning about the Dynamic Context of a Still Image,"In-Context-Learning, VQA",https://arxiv.org/abs/2004.10796,,,2020/04/22,Reasoning
,ChatEval,ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,In-Context-Learning,https://arxiv.org/abs/2308.07201,,,2023/08/14,Memory
,Generative Agents,Generative Agents: Interactive Simulacra of Human Behavior,In-Context-Learning,https://arxiv.org/abs/2304.03442,,,2023/04/07,Memory
,Self-supervised ICL,SINC: Self-Supervised In-Context Learning for Vision-Language Tasks,"In-Context-Learning, VQA",https://arxiv.org/abs/2307.07742,,,2023/07/15,Self-supervised
,BIG-Bench,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,In-Context-Learning,https://arxiv.org/abs/2206.04615,,,2022/06/09,Benchmark
,ARB,ARB: Advanced Reasoning Benchmark for Large Language Models,"Benchmark, In-Context-Learning",https://arxiv.org/abs/2307.13692,,,2023/07/25,Benchmark
,PlanBench,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Benchmark, In-Context-Learning",https://arxiv.org/abs/2206.10498,,,2022/06/21,Benchmark
,,War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars,"Agent, Multi",https://arxiv.org/abs/2311.17227,,,,
,AppAgent,AppAgent: Multimodal Agents as Smartphone Users,"Agent, GUI, MobileApp",,,,,
,MindAgent,MindAgent: Emergent Gaming Interaction,Agent,,,,,
,,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, Embodied",,,,,
,,STARLING: SELF-SUPERVISED TRAINING OF TEXTBASED REINFORCEMENT LEARNING AGENT WITH LARGE LANGUAGE MODELS,"Agent, Reinforcement-Learning",,,,,
,InfiAgent,InfiAgent: A Multi-Tool Agent for AI Operating Systems,Agent,,,,,
,,Predictive Minds: LLMs As Atypical Active Inference Agents,Agent,,,,,
,,FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation,"RAG, Temporal Logics",https://arxiv.org/abs/2310.03214,,,,
,,LoRA: Low-Rank Adaptation of Large Language Models,"LoRA, Scaling",,,,,
,,Large Language Models for Information Retrieval: A Survey,"RAG, Survey",,,,,
,,TinyLLaVA: A Framework of Small-scale Large Multimodal Models,"LLaVA, VLM",https://arxiv.org/abs/2402.14289,,,,
,,Recognize Anything: A Strong Image Tagging Model,Perception,https://arxiv.org/abs/2306.03514,,,,
,OmniACT,OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web,"Agent, Web",https://arxiv.org/abs/2402.17553,,,,
,,Video as the New Language for Real-World Decision Making,"Agent, Video-for-Agent",,,,,
,,LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,"Agent, LLM, Planning",,,,,
,,swarms,Agent,,https://github.com/kyegomez/swarms,,,
,,Gemma: Introducing new state-of-the-art open models,Open-source,https://blog.google/technology/developers/gemma-open-models/,,,,
,,Awesome-Diffusion-Models,"Awesome Repo, Diffusion",,https://github.com/diff-usion/Awesome-Diffusion-Models,,,
,,Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots,"Robot, Zero-shot",,,,,
,,Chain-of-Thought Reasoning Without Prompting,Reasoning,https://arxiv.org/abs/2402.10200,,,,
,,MM-LLMs: Recent Advances in MultiModal Large Language Models,"Survey, VLM",,,,,
,,ReFT: Reasoning with Reinforced Fine-Tuning,"Reasoning, Reinforcement-Learning",,,,,
,,Generative Expressive Robot Behaviors using Large Language Models,Robot,,,,,
,,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","Awesome Repo, Hallucination, Survey",https://arxiv.org/abs/2311.05232,https://github.com/LuckyyySTA/Awesome-LLM-hallucination,,,
,,Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,"Context-Window, Scaling",,,,,
,,Advances in 3D Generation: A Survey,"Generation, Survey",,,,,
,,A Survey on Evaluation of Large Language Models,"Evaluation, LLM, Survey",https://arxiv.org/abs/2307.03109,,,,
,,DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models,"Math, Reasoning",,,,,
,,Contrastive Chain-of-Thought Prompting,Reasoning,,,,,
,,"Retrieval-Augmented Generation for Large Language ","RAG, Survey",,,,,
,,Rephrase and Respond(RaR),Reasoning,,,,,
,,Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models,Reasoning,,,,,
,,OS-Copilot: Towards Generalist Computer Agents with Self-Improvement,"Agent, Web",https://arxiv.org/abs/2402.07456,,,,
,,Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models,"Generation, Robot, Zero-shot",https://arxiv.org/abs/2310.10639,,,,
,,Towards Generalizable Zero-Shot Manipulationvia Translating Human Interaction Plans,"Generation, Robot, Zero-shot",,,,,
80,,Language Models as Zero-Shot Trajectory Generators,"LLM, PersonalCitation, Robot, Zero-shot",https://arxiv.org/abs/2310.11604,,,,
,,Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting,"Robot, Zero-shot",,,,,
,,Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,"Robot, Zero-shot",,,,,
,,Large Language Models are Zero-Shot Reasoners,"Reasoning, Zero-shot",,,,,
,,Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?,Zero-shot,,,,,
,,Code as Reward: Empowering Reinforcement Learning with VLMs,"Code-as-Policies, Reinforcement-Learning, Reward",https://arxiv.org/abs/2402.04764,,,,
,,"Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models","Sora, Text-to-Video",,,,,
,,MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT,"LLaMA, Lightweight, Open-source",,,,,
,,TaskWeaver: A Code-First Agent Framework,"Agent, Code-LLM",,,,,
,,LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models01,"Benchmark, Sora, Text-to-Video",,,,,
,,EgoCOT: Embodied Chain-of-Thought Dataset for Vision Language Pre-training,"Chain-of-Thought, Embodied, Robot",,,,,
,,Language Segment-Anything,"Perception, Robot",,,,,
,,Large World Model (LWM)01,"VLM, World-model",,,,,
,,Learning and Leveraging World Models in Visual Representation Learning,World-model,,,,,
,,Resonance RoPE: Improving Context Length Generalization of Large Language Models,"Context-Window, Reasoning, RoPE, Scaling",,,,,
,,RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation,Robot,,,,,
,,RoFormer: Enhanced Transformer with Rotary Position Embedding,RoPE,https://arxiv.org/abs/2104.09864,,,,
,,Language Models Meet World Models: Embodied Experiences Enhance Language Models,"Embodied, World-model",,,,,
,,[Resource] Paperswithcode,Resource,https://paperswithcode.com/,,,,
,,[Resource] huggingface,Resource,https://huggingface.co/papers,,,,
,,Gorilla: Large Language Model Connected with Massive APIs,"Agent, Tool",,,,,
,,AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation,"Reasoning, Robot",,,,,
,,Introspective Tips: Large Language Model for In-Context Decision Making,Robot,https://www.semanticscholar.org/paper/Introspective-Tips%3A-Large-Language-Model-for-Making-Chen-Wang/047e3812854a86b2a2e113219fa956eda860ce24,,,,
90,,Multimodal & Large Language Models,"Awesome Repo, LLM, VLM",,https://github.com/Yangyi-Chen/Multimodal-AND-Large-Language-Models,,,
,,A Survey of Reinforcement Learning from Human Feedback,"RLHF, Reinforcement-Learning, Survey",,,,,
,,Instruction Tuning for Large Language Models: A Survey,"Instruction-Turning, LLM, Survey",,,,,
,,Agent AI: Surveying the Horizons of Multimodal Interaction,"Agent, Survey",,,,,
,,Large Multimodal Agents: A Survey,"Agent, Survey",,,,,
,,Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning,"Reasoning, Symbolic",,,,,
,,[Resource] dailyarxiv,Resource,https://dailyarxiv.com/,,,,
,,You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, MobileApp",,https://github.com/cooelf/Auto-UI,,,
,,Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning,"Agent, Reasoning",,,,,
,,Large Language Models for Robotics: A Survey,"LLM, Robot, Survey",,,,,
,,RoboCodeX:Multi-modal Code Generation forRobotic Behavior Synthesis,"Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2402.16117,,,,
,,Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld,"Agent, Embodied",https://arxiv.org/abs/2311.16714,,,,
,,Octopus: Embodied Vision-Language Programmer from Environmental Feedback,"Agent, Embodied",,,,,
,,Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Code-as-Policies, Reasoning",,,,,
,,Executable Code Actions Elicit Better LLM Agents,"Agent, Code-as-Policies",https://arxiv.org/abs/2402.01030,,,2024/01/24,
,,Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning,"Instruction-Turning, Survey",,,,,
,,Vision-Language Instruction Tuning: A Review and Analysis,"Instruction-Turning, Survey",,,,,
,,A Closer Look at the Limitations of Instruction Tuning,Instruction-Turning,,,,,
,,REVO-LION: EVALUATING AND REFINING VISION LANGUAGE INSTRUCTION TUNING DATASETS,"Datatset, Instruction-Turning",,,,,
,,INSTRUCTION TUNING WITH GPT-4,"GPT4, Instruction-Turning",https://arxiv.org/abs/2304.03277,,,,
,,Exploring Format Consistency for Instruction Tuning,Instruction-Turning,,,,,
,,Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models,"Datatset, Instruction-Turning",,,,,
,,[Resource] Connectedpapers,Resource,https://www.connectedpapers.com/,,,,
,,RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents,"Agent, Memory, RAG",https://arxiv.org/abs/2402.03610,,,2024/02/06,
,,Training Language Models with Memory Augmentation,RAG,,,,,
,,Language Models Meet World Models,World-model,,,,,
,,Learning to Model the World with Language,World-model,,,,,
,,Diffusion World Model,World-model,,,,,
,,PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs,Robot,https://arxiv.org/abs/2402.07872,,,,
,,Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought,"Chain-of-Thought, Code-as-Policies, PersonalCitation, Robot",https://arxiv.org/abs/2305.16744,,,,
,,RT-H: Action Hierarchies Using Language,"Natural-Language-as-Polices, Robot",https://arxiv.org/abs/2403.01823,,,,
,,Application of Pretrained Large Language Models in Embodied Artificial Intelligence,"Agent, Embodied, Survey",https://www.semanticscholar.org/paper/Application-of-Pretrained-Large-Language-Models-in-Kovalev-Panov/04f87baf7d1b3eb303a52a8a66c8189f396dd114,,,,
,,Simple Open-Vocabulary Object Detection with Vision Transformers,Perception,https://arxiv.org/abs/2205.06230,,,,
,,ScreenAgent: A Vision Language Model-driven Computer Control Agent,Agent,,,,,
70,,XLang Paper Reading,"Agent, Awesome Repo, Embodied, Grounding",,https://github.com/xlang-ai/xlang-paper-reading,,,
,,Executable Code Actions Elicit Better LLM Agents,"Code-as-Policies, Robot",,,,,
,,Learning to Model the World with Language,World-model,,,,,
,,Secrets of RLHF in Large Language Models Part I: PPO,"PPO, RLHF, Reinforcement-Learning",https://arxiv.org/abs/2402.01030,,,2024/02/01,
,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,"Context-Window, Foundation, Gemini, LLM, Scaling",,,,,
,,"LONGNET: Scaling Transformers to 1,000,000,000 Tokens",Scaling,https://arxiv.org/abs/2307.02486,,,2023/07/01,
,,STaR: Bootstrapping Reasoning With Reasoning,Reasoning,https://arxiv.org/abs/2203.14465,,,2022/05/28,
,,The Impact of Reasoning Step Length on Large Language Models,Reasoning,,,,,
,,A Closer Look at the Limitations of Instruction Tuning,"Instruction-Turning, Survey",https://arxiv.org/abs/2402.05119,,,,
,,OpenAGI: When LLM Meets Domain Experts,"AGI, Agent",,,54,,
,,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",Agent,,,,,
,,Exploring the Benefits of Training Expert Language Models over Instruction Tuning,Instruction-Turning,https://arxiv.org/abs/2302.03202,,27,2023/02/06,
,,In-Context Instruction Learning,"In-Context-Learning, Instruction-Turning",,,,,
,,Tuna: Instruction Tuning using Feedback from Large Language Models,Instruction-Turning,https://arxiv.org/abs/2310.13385,,,2023/03/06,
,,"Structured Prompting: Scaling In-Context Learning to 1,000 Examples","In-Context-Learning, Scaling",,,20,2020/03/06,
,,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,"In-Context-Learning, Scaling",,,18,2022/03/06,
,,Secrets of RLHF in Large Language Models Part II: Reward Modeling,RLHF,,,,,
,,CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval,"Perception, Video, Vision",https://arxiv.org/abs/2104.08860,,,,
,,LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models,"Image, LLaMA, Perception",,,,,
,,SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding,Perception,,,,,
,,Visual In-Context Prompting,"In-Context-Learning, Perception, Vision",,,,,
,,What does CLIP know about a red circle? Visual prompt engineering for VLMs,In-Context-Learning,,,,,
,,What Makes Good Examples for Visual In-Context Learning?,"In-Context-Learning, Vision",,,,,
,,Visual Prompting via Image Inpainting,"In-Context-Learning, Vision",,,,,
,,Prompting Visual-Language Models for Efficient Video Understanding,"In-Context-Learning, Video",,,,,
,,Visual Prompt Tuning,"In-Context-Learning, Prompt-Tuning",,,,,
,,Visually Grounded Reasoning across Languages and Cultures,"Grounding, Reasoning",,,,,
,,V-IRL: Grounding Virtual Intelligence in Real Life,Grounding,,,,,
,,GLaMM: Pixel Grounding Large Multimodal Model,Grounding,,,,,
,,Awesome Embodied Vision,"Awesome Repo, Embodied",,https://github.com/ChanganVR/awesome-embodied-vision,,,
,,PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization,Agent,,,,,
,,Autonomous Agents,"Agent, Awesome Repo",,https://github.com/tmgthb/Autonomous-Agents,,,
,,3D Diffusion Policy,"Diffusion, Robot",https://arxiv.org/abs/2403.03954,,,,
,,Real-World Robot Applications of Foundation Models: A Review,"Robot, Survey",,,,,
,,RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation,"Code-as-Policies, Robot",,,,,
,,Creative Robot Tool Use with Large Language Models,"Code-as-Policies, Robot",,,,,
,,RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks,"Code-as-Policies, PersonalCitation, Robot",,,,,
,,ChatGPT for Robotics: Design Principles and Model Abilities,"Code-as-Policies, PersonalCitation, Robot",,,,,
,,DeepWisdom - MetaGPT,Lab,,,,,
,,Reworkd AI - AgentGPT,Lab,,,,,
,,"OpenBMB - ChatDev, XAgent, AgentVerse",Lab,,,,,
,,XLANG NLP Lab - OpenAgents,Lab,,,,,
,,Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation,"Chain-of-Thought, Reasoning",,,,,
,,"Rutgers University, AGI Research - OpenAGI",Lab,,,,,
,,Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University -  CogVLM,Lab,,,,,
,,Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds,"Agent, Minecraft",,,,,
,,You Only Look at Screens: Multimodal Chain-of-Action Agents,"Agent, GUI, MobileApp",,,,,
,,"LEARNING EMBODIED VISION-LANGUAGE PRO- GRAMMING FROM INSTRUCTION, EXPLORATION, AND ENVIRONMENTAL FEEDBACK","Agent, Game",,,,,
,,Embodied Task Planning with Large Language Models,"Agent, Embodied",,,,,
,,Can Language Agents Approach the Performance of RL? An Empirical Study On OpenAI Gym,"Gym, PPO, Reinforcement-Learning, Survey",,,,,
,,AGENT INSTRUCTS LARGE LANGUAGE MODELS TO BE GENERAL ZERO-SHOT REASONERS,"Agent, Reasoning",,,,,
,,ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs,"Agent, Tool",,,,,
,,LLM Powered Autonomous Agents,"Agent, Blog",https://lilianweng.github.io/posts/2023-06-23-agent/,,,,
,,ScreenAgent: A Computer Control Agent Driven by Visual Language Large Model,"Agent, GUI",,https://github.com/niuzaisheng/ScreenAgent,,,
,,SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents,"Agent, GUI",,,,,
,,"""What’s important here?"": Opportunities and Challenges of Using LLMs in Retrieving Informatio from Web Interfaces","Agent, GUI, Web",,,,,
,,Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Instruction-Turning, Self",,,817,,
,,[Resource] Semanticscholar,Resource,https://www.semanticscholar.org,,,,
,,A Survey on In-context Learning,"In-Context-Learning, Survey",https://arxiv.org/abs/2301.00234,,,,
,,Combating Misinformation in the Age of LLMs: Opportunities and Challenges,"Hallucination, Survey",https://arxiv.org/abs/2311.05656,,,,
,,Awesome-Papers-Autonomous-Agent,"Agent, Awesome Repo",,https://gh.mlsub.net/lafmdp/Awesome-Papers-Autonomous-Agent,,,
,,Understanding LLMs: A Comprehensive Overview from Training to Inference,"Survey, Training",,,3,,
,,A Survey on Data Selection for LLM Instruction Tuning,"Instruction-Turning, Survey",,,,,
,,"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","Agent, Code-LLM, Code-as-Policies, Survey",https://arxiv.org/abs/2401.00812,,5,,
,,A Survey on Knowledge Distillation of Large Language Models,"Distilling, Survey",,,,,
,,A Survey on Data Selection for Language Models,"Datatset, LLM, Survey",,,,,
100,,Awesome-LLM-Survey,"Awesome Repo, LLM, Survey",,https://github.com/HqWu-HITCS/Awesome-LLM-Survey,,,
70,,Awesome Large Multimodal Agents,"Agent, Awesome Repo",,https://github.com/jun0wanan/awesome-large-multimodal-agents,,,
,,Awesome Vision-Language Navigation,"Awesome Repo, Perception, VLM",,https://github.com/daqingliu/awesome-vln,,,
,,Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook,"Survey, TimeSeries",,,,,
,,OpenGVLab,Lab,,https://github.com/OpenGVLab,,,
,,Imperial College London - Zeroshot trajectory,Lab,,,,,
,,[Resource] AlphaSignal,Resource,https://alphasignal.ai/,,,,
,,[Resource] arxiv-sanity,Resource,https://arxiv-sanity-lite.com/,,,,
,,InCoRo: In-Context Learning for Robotics Control with Feedback Loops,"Feedback, In-Context-Learning, Robot",,,,,
,,S-Agents: Self-organizing Agents in Open-ended Environment,"Agent, Minecraft",,,,,
,,OCI-Robotics: Object-Centric Instruction Augmentation for Robotic Manipulation,Robot,https://arxiv.org/abs/2401.02814,,,,
,,Lenna: Language Enhanced Reasoning Detection Assistant,"Perception, Reasoning",https://arxiv.org/abs/2312.02433,,,,
,,Could a Large Language Model be Conscious?,"Brain, Conscious",,,,,
,,Could a Large Language Model be Conscious?,"Brain, Conscious",,,,,
,,A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization,Brain,,,,,
,,(Long)LLMLingua: Enhancing Large Language Model Inference via Prompt Compression,"Compress, Scaling",https://arxiv.org/abs/2310.05736,,,,
,,LlamaIndex,Package,,https://github.com/run-llama/llama_index,,,
,,LangChain,Package,,https://github.com/langchain-ai/langchain,,,
,,h2oGPT,Package,,https://github.com/h2oai/h2ogpt,,,
60,,Awesome LLMOps,"Awesome Repo, Package",,https://github.com/tensorchord/Awesome-LLMOps,,,
,,Dify,Package,,https://github.com/langgenius/dify,,,
,,Alpaca-LoRA,Package,,https://github.com/tloen/alpaca-lora,,,
,,Efficient Large Language Models: A Survey,Survey,https://arxiv.org/abs/2312.03863,https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey,,,
,,Robust Speech Recognition via Large-Scale Weak Supervision,Audio,,,,,
,,A latent text-to-image diffusion model,Diffusion,,,,,
,,A Survey on Model Compression for Large Language Models,"Compress, Quantization, Survey",https://arxiv.org/abs/2308.07633,,,,
30,,Awesome LLM Compression,"Awesome Repo, Compress",,https://github.com/HuangOwen/Awesome-LLM-Compression,,,Awesome LLM Compression
,40,Cognitive Architectures for Language Agents,Agent,https://arxiv.org/abs/2309.02427,,,,
,,LLM Agents Papers,"Agent, Awesome Repo",,https://github.com/zjunlp/LLMAgentPapers,,,
,,Awesome LLM-Powered Agent,"Agent, Awesome Repo",,https://github.com/hyp1231/awesome-llm-powered-agent,,,
,,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,"LoRA, Scaling",https://arxiv.org/abs/2305.03695,,,,
,,Video Understanding with Large Language Models: A Survey,"Survey, Video",,,,,
,,A Survey on Multimodal Large Language Models for Autonomous Driving,"Drive, Survey",https://arxiv.org/abs/2311.12320,,,,
,,Retrieval-Augmented Generation for Large Language Models: A Survey,"RAG, Survey",,,,,
,,On the Design Fundamentals of Diffusion Models: A Survey,"Diffusion, Survey",https://arxiv.org/abs/2306.04542,,,,
,,Chain-of-table: Evolving tables in the reasoning chain for table understanding,"Chain-of-Thought, Reasoning, Table",,,,,
,,DeepSeek-VL: Towards Real-World Vision-Language Understanding01,"VLM, VQA",,,,,
,,Tree-Planner: Efficient Close-loop Task Planning with Large Language Models01,"LLM, PersonalCitation, Robot",,,,,
,,GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration,"Demonstration, GPT4, PersonalCitation, Robot",,,,,
,,IROS2023PaperList,"Awesome Repo, IROS, Robot",,https://github.com/gonultasbu/IROS2023PaperList,,,
,,Paper List for In-context Learning,"Awesome Repo, In-Context-Learning",,https://github.com/dqxiu/ICL_PaperList,,,
,,OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following,"Agent, Embodied, Robot",,,,,
,,Chain of Code: Reasoning with a Language Model-Augmented Code Emulator,"Chain-of-Thought, Code-as-Policies",https://arxiv.org/abs/2312.04474,,,,
,,AgentTuning: Enabling Generalized Agent Abilities For LLMs,"Agent, Instruction-Turning",https://arxiv.org/abs/2310.12823,,,,Comprehensive Collection of LLM-Related Papers
,,Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory01,"Agent, Minecraft",,,,,
,,ScreenAI: A Vision-Language Model for UI and Infographics Understanding,VLM,,,,,
,,Prompt a Robot to Walk with Large Language Models,"Action-Generation, Generation, Prompting",,,,,
,,Can large language models explore in-context?,In-Context-Learning,,,,,
,,RoFormer: Enhanced Transformer with Rotary Position Embedding,Context-Window,,,,,
,,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,"Context-Window, Foundation",,,,,
,,Corrective Retrieval Augmented Generation,"CRAG, RAG",https://arxiv.org/abs/2401.15884,,,,
,,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",RAG,,,,,
,,RAG-Fusion: a New Take on Retrieval-Augmented Generation,RAG,https://arxiv.org/abs/2402.03367,,,,
,,Gorilla: Large Language Model Connected with Massive APIs,"APIs, Agent, Tool",https://arxiv.org/abs/2305.15334,,,,
,,RAFT: Adapting Language Model to Domain Specific RAG,RAG,https://arxiv.org/abs/2403.10131,,,,
,,AIOS: LLM Agent Operating System,Agent,https://arxiv.org/abs/2403.16971,,,,
,,"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",Agent,https://arxiv.org/abs/2312.03815,,,,
,,Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity,RAG,https://arxiv.org/abs/2403.14403,,,,
,,"DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for Adaptive and Minimally Deforming Grasp Policies",Robot,https://arxiv.org/abs/2403.07832,,,,
,,LaVague,"Action-Model, Agent, LAM",,https://github.com/lavague-ai/LaVague,,,
,,Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation,Tex2Img,https://arxiv.org/abs/2403.16990,,,,
,,Awesome-Chinese-LLM,"Awesome Repo, Chinese",,https://github.com/HqWu-HITCS/Awesome-Chinese-LLM,,,
,,awesome-korean-llm,"Awesome Repo, Korean",,https://github.com/NomaDamas/awesome-korean-llm,,,
,,InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding,"ViFM, Video",https://arxiv.org/abs/2403.15377,https://github.com/OpenGVLab/InternVideo2/,,,
,,Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,"Sora, Text-to-Video",https://arxiv.org/abs/2403.13248,,,,